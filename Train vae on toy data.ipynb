{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm.auto import tqdm\n",
    "import GPUtil\n",
    "import numpy as np\n",
    "from utils.vmf_batch import vMF\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2SeqDataSet, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from itertools import product\n",
    "from utils.training_utils import create_Seq2SeqDataset, train, evaluate\n",
    "from importlib import reload\n",
    "import utils.training_utils\n",
    "\n",
    "from datetime import datetime\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(training_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "INPUT_DIM = 3   \n",
    "lr = 1e-2                           # learning rate\n",
    "NUM_LAYERS = 2\n",
    "NUM_CLASSES = 3\n",
    "N_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/toy_data/3_populations/iterator/val_iterator.pkl', 'rb') as f:\n",
    "    val_iterator = pickle.load(f)\n",
    "\n",
    "with open('./data/toy_data/3_populations/iterator/train_iterator.pkl', 'rb') as f:\n",
    "    train_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)\n",
    "MASKING_ELEMENT = train_iterator.dataset.masking_el\n",
    "n_walks = train_iterator.dataset.n_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMD_D = HID_D = 16, LAT_D = 8, Dropout= 0.1, kappa= 500, pooling=avg\n",
    "# EMD_D = HID_D = 16, LAT_D = 8, Dropout= 0.3, kappa= 500, pooling=avg\n",
    "# EMD_D = HID_D = 16, LAT_D = 8, Dropout= 0.5, kappa= 500, pooling=avg\n",
    "embedding_dims = [16,32]\n",
    "latent_dims = [8,16,32]\n",
    "dropout = [.5]\n",
    "kappa = [500]\n",
    "pooling = ['avg', 'max']\n",
    "parameter_grid = list(product(embedding_dims, latent_dims, dropout, kappa, pooling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for ./models/parameter_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(parameter_grid[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6e7c69bef341018a642a854a9e93f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with parameters:     EMD_D = HID_D = 16, LAT_D = 8, Dropout= 0.5, kappa= 500, pooling=avg\n",
      "KLD: 18.465579986572266\n"
     ]
    }
   ],
   "source": [
    "# for each parameter combination, train 3 models\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "n_runs = 1\n",
    "pbar = tqdm(total=n_runs*len(parameter_grid))\n",
    "\n",
    "while len(parameter_grid) > 0:\n",
    "    \n",
    "    # get next parameter set\n",
    "    emb_dim = int(parameter_grid[0][0])\n",
    "    latent_dim = int(parameter_grid[0][1])\n",
    "    dpout = float(parameter_grid[0][2])\n",
    "    kappa = int(parameter_grid[0][3])\n",
    "    pool = parameter_grid[0][4]\n",
    "    \n",
    "    print('Fitting model with parameters: \\\n",
    "    EMD_D = HID_D = %i, LAT_D = %i, Dropout= %.1f, kappa= %i, pooling=%s'%(emb_dim, \n",
    "                                                                           latent_dim, \n",
    "                                                                           dpout, \n",
    "                                                                           kappa, \n",
    "                                                                           pool))\n",
    "    # save the file without this parameter set\n",
    "    with open('./models/parameter_search/parameter_grid.txt', 'wb') as f: \n",
    "        np.save(f,parameter_grid[1:])\n",
    "        \n",
    "\n",
    "    for k in range(n_runs):\n",
    "        pbar.update()\n",
    "        \n",
    "        start = datetime.now()\n",
    "        # model\n",
    "        enc = SeqEncoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dec = SeqDecoder(INPUT_DIM, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "        dist = vMF(latent_dim, kappa=kappa)\n",
    "        model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "        classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)\n",
    "        \n",
    "        # initialize model \n",
    "        model.apply(init_weights)\n",
    "        classifier.apply(init_weights)\n",
    "        \n",
    "        # losses\n",
    "        cross_entropy_loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "        mse_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        #optimizer\n",
    "        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "\n",
    "        ### train the model(s)\n",
    "\n",
    "        best_test_loss = np.infty\n",
    "        training = []\n",
    "        validation = []\n",
    "        for e in range(N_EPOCHS):\n",
    "\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, 1,1.)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss)\n",
    "\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "\n",
    "            training += [[train_loss, train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            #print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {val_loss:.2f}')\n",
    "\n",
    "\n",
    "            if e % 50 == 0 and e > 0:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "            \n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                suffix = 'emb%i_hid%i_lat%i_dp%.1f_k%i_%s'%(emb_dim,emb_dim,latent_dim,dpout,kappa,pool)\n",
    "                torch.save({'epoch': e,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'classifier_state_dict': classifier.state_dict()\n",
    "                           }, './models/parameter_search/%s_run%i_best.pt'%(suffix,(k+1)))\n",
    "        # save training and validation loss\n",
    "        validation_ = np.array(validation)\n",
    "        training_ = np.array(training)\n",
    "        losses = np.concatenate((training_, validation_), axis=1)\n",
    "        # losses [:,0] = training loss, [:,1] = training classification loss\n",
    "        # [:,2] = validation loss, [:,3] = validation classification loss\n",
    "        with open('./models/parameter_search/losses_%s_%i.npy'%(suffix, (k+1)), 'wb') as f:\n",
    "            np.save(f,losses)\n",
    "            \n",
    "        end = datetime.now()\n",
    "        print('Time to fit model %i : '%(k+1), end-start)\n",
    "    torch.cuda.empty_cache()\n",
    "    with open('./models/parameter_search/parameter_grid.txt', 'rb') as f: \n",
    "        parameter_grid = np.load(f)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
