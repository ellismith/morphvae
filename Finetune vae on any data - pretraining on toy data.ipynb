{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from utils.vmf_batch import vMF\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from utils.training_utils import train, evaluate\n",
    "\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting directories for the dataset's data iterators\n",
    "iterator_path = Path('./data/Farrow_data/iterator/soma_centered')\n",
    "train_iterator_path = iterator_path / 'train_iterator.pkl'\n",
    "val_iterator_path = iterator_path / 'val_iterator.pkl'\n",
    "\n",
    "with open(train_iterator_path, 'rb') as f:\n",
    "    train_iterator = pickle.load(f)\n",
    "\n",
    "with open(val_iterator_path, 'rb') as f:\n",
    "    val_iterator = pickle.load(f)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('./models/Farrow/finetuned/soma_centered')\n",
    "state_dict_path = Path('./models/parameter_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data, trg_data, seq_len, indices, labels = list(train_iterator)[0]\n",
    "bs, n_walks, walk_length, output_dim = src_data.shape\n",
    "\n",
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)\n",
    "\n",
    " \n",
    "MASKING_ELEMENT = train_iterator.dataset.masking_el\n",
    "\n",
    "# get number of labels, ignore -100 index\n",
    "l = list(np.unique(labels))\n",
    "if -100 in l:\n",
    "    l.remove(-100)\n",
    "NUM_CLASSES = len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 18.465579986572266\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 16\n",
    "hid_dim = 16 # idk if i need this or not\n",
    "latent_dim = 8\n",
    "NUM_LAYERS = 2\n",
    "dpout = .1\n",
    "kap = 500\n",
    "pool = 'avg'\n",
    "lr = 0.01\n",
    "\n",
    "enc = SeqEncoder(output_dim, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "dec = SeqDecoder(output_dim, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "dist = vMF(latent_dim, kappa=kap)\n",
    "model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"./models/5_populations\"\n",
    "#import os \n",
    "#os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_combo = 'emb%s_hid%s_lat%s_dp%s_k%s_%s'%(emb_dim, hid_dim, latent_dim,\n",
    "                                                dpout, kap, pool)\n",
    "state_dict_pt = '%s_run1_best.pt'%param_combo\n",
    "#state_dict = torch.load(state_dict_path / state_dict_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('models/parameter_search/emb16_hid16_lat8_dp0.1_k500_avg_run1_best.pt')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict_path / state_dict_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ellismith/morphvae/Finetune vae on any data - pretraining on toy data.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=41'>42</a>\u001b[0m validation \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(losses[:,\u001b[39m2\u001b[39m:])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=44'>45</a>\u001b[0m     start\u001b[39m.\u001b[39;49mrecord()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=45'>46</a>\u001b[0m     train_loss, train_class_loss \u001b[39m=\u001b[39m train(model, classifier, train_iterator, optimizer, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=46'>47</a>\u001b[0m                                        calculate_loss,cross_entropy_loss, clip\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, norm_p\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=47'>48</a>\u001b[0m                                          class_fraction\u001b[39m=\u001b[39mfrac)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=48'>49</a>\u001b[0m     val_loss, val_class_loss \u001b[39m=\u001b[39m evaluate(model,classifier, val_iterator,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/morphvae/Finetune%20vae%20on%20any%20data%20-%20pretraining%20on%20toy%20data.ipynb#ch0000012vscode-remote?line=49'>50</a>\u001b[0m                                          calculate_loss, cross_entropy_loss, norm_p\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py:153\u001b[0m, in \u001b[0;36mEvent.record\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=147'>148</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Records the event in a given stream.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=148'>149</a>\u001b[0m \n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=149'>150</a>\u001b[0m \u001b[39mUses ``torch.cuda.current_stream()`` if no stream is specified. The\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=150'>151</a>\u001b[0m \u001b[39mstream's device must match the event's device.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=151'>152</a>\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=152'>153</a>\u001b[0m     stream \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_stream()\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/streams.py?line=153'>154</a>\u001b[0m \u001b[39msuper\u001b[39m(Event, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrecord(stream)\n",
      "File \u001b[0;32m/opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py:427\u001b[0m, in \u001b[0;36mcurrent_stream\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=417'>418</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_stream\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Stream:\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=418'>419</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the currently selected :class:`Stream` for a given device.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=419'>420</a>\u001b[0m \n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=420'>421</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=424'>425</a>\u001b[0m \u001b[39m            (default).\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=425'>426</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=426'>427</a>\u001b[0m     _lazy_init()\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=427'>428</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Stream(_cdata\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getCurrentStream(\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=428'>429</a>\u001b[0m         _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n",
      "File \u001b[0;32m/opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py:170\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=165'>166</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=166'>167</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=167'>168</a>\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=168'>169</a>\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=169'>170</a>\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=172'>173</a>\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/mambaforge/lib/python3.9/site-packages/torch/cuda/__init__.py?line=173'>174</a>\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "\n",
    "N_EPOCHS= 200\n",
    "save_path_model= model_path / 'finetuned_vae_frac%.1f_best_run%i.pt'\n",
    "save_path_losses = model_path / 'finetuned_losses_frac%.1f_run%i.npy'\n",
    "save_path_elapsed_time = model_path / 'finetuned_elapsed_time_frac%.1f_run%i.npy'\n",
    "\n",
    "param_combo = 'emb%s_hid%s_lat%s_dp%s_k%s_%s'%(emb_dim, hid_dim, latent_dim,\n",
    "                                                dpout, kap, pool)\n",
    "#state_dict_pt = '%s_frac1.0_run1_best.pt'%param_combo\n",
    "#state_dict_pt = '%s_run1_best.pt'%param_combo\n",
    "state_dict_pt = '%s_run1.pt'%param_combo\n",
    "state_dict = torch.load(state_dict_path / state_dict_pt)\n",
    "state_dict_npy = '%s_run1.npy'%param_combo\n",
    "\n",
    "# start with the 5 pop toy data, \"true\" finetuning on the Farrow data\n",
    "# loads in parameters from 5 pop toy data, updates from what it learns from Farrow\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for frac in [1., .9, .5, .1, 0.]:\n",
    "    \n",
    "  \n",
    "    runs = range(1,4)\n",
    "        \n",
    "    for run in runs:\n",
    "        \n",
    "        # load pre-trained model\n",
    "        # the first run was the best\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        classifier.apply(init_weights)\n",
    "        cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=-100)\n",
    "        mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "        #optimizer\n",
    "        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "       \n",
    "        best_test_loss = np.infty\n",
    "\n",
    "        losses = np.load(state_dict_path / state_dict_npy)\n",
    "        elapsed_time = np.zeros((N_EPOCHS))\n",
    "        training = list(losses[:,:2])\n",
    "        validation = list(losses[:,2:])\n",
    "        \n",
    "        for e in range(N_EPOCHS):\n",
    "            start.record()\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, clip=1, norm_p=None,\n",
    "                                                 class_fraction=frac)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss, norm_p=None)\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "            \n",
    "            end.record()\n",
    "\n",
    "            # Waits for everything to finish running\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time[e] = start.elapsed_time(end) # milliseconds\n",
    "            \n",
    "            training += [[train_loss,train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, Time elapsed [s]: {elapsed_time[e]/1000:.2f}')\n",
    "\n",
    "\n",
    "            if e % 50 == 0 and e > 0:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "\n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                torch.save({'epoch': e,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'classifier_state_dict': classifier.state_dict()\n",
    "                               },save_path_model%(frac, run))\n",
    "\n",
    "                validation_ = np.array(validation)\n",
    "                training_ = np.array(training)\n",
    "                # [:,0] = training loss, [:,1] = training classification loss \n",
    "                # [:,2] validation loss, [:,3] validation classification loss\n",
    "                losses = np.hstack((training_, validation_))\n",
    "                np.save(save_path_losses%(frac, run),losses)\n",
    "                np.save(save_path_elapsed_time%(frac,run),elapsed_time)\n",
    "        validation = np.array(validation)\n",
    "        training = np.array(training)\n",
    "        losses = np.hstack((training, validation))\n",
    "        np.save(save_path_losses%(frac, run), losses)\n",
    "        np.save(save_path_elapsed_time%(frac,run),elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(validation)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time.mean()/1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
