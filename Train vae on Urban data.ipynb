{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.vmf_batch import vMF\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from utils.training_utils import train, evaluate\n",
    "\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X,scale=(.1,3)):\n",
    "    a = scale[0]\n",
    "    b = scale[1]\n",
    "    s = a + (b-a)*torch.rand(1)\n",
    "    return X*s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_train = './data/urban_data/iterator/train_iterator.pkl'\n",
    "path_val = './data/urban_data/iterator/val_iterator.pkl'\n",
    "PATH_train = Path(path_train).mkdir(parents=True, exist_ok=True)\n",
    "PATH_val = Path(path_val).mkdir(parents=True, exist_ok=True)\n",
    "#PATH = './data/urban_data/iterator/soma_centered/scaling_train_iterator.pkl'\n",
    "\n",
    "\n",
    "#os.makedirs(PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './data/urban_data/iterator/train_iterator.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ellismith/GraNNule/morphvae/Train vae on Urban data.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m#path = './data/%s/iterator/%s/%s%s.pkl'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m#part = 'soma_centered'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m#folder = 'urban_data'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m#prefix = 'scaling_'\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_train, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=6'>7</a>\u001b[0m     train_iterator \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bspike/home/ellismith/GraNNule/morphvae/Train%20vae%20on%20Urban%20data.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path_val, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './data/urban_data/iterator/train_iterator.pkl'"
     ]
    }
   ],
   "source": [
    "#path = './data/%s/iterator/%s/%s%s.pkl'\n",
    "#part = 'soma_centered'\n",
    "#folder = 'urban_data'\n",
    "#prefix = 'scaling_'\n",
    "\n",
    "with open(path_train, 'rb') as f:\n",
    "    train_iterator = pickle.load(f)\n",
    "    \n",
    "with open(path_val, 'rb') as f:\n",
    "    val_iterator = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data, trg_data, seq_len, indices, labels = list(train_iterator)[0]\n",
    "bs, n_walks, walk_length, output_dim = src_data.shape\n",
    "\n",
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "latent_dim = 32\n",
    "NUM_LAYERS = 2\n",
    "dpout = .1\n",
    "kap = 500\n",
    "pool = 'max'\n",
    "lr = 0.01\n",
    "MASKING_ELEMENT = train_iterator.dataset.masking_el\n",
    "\n",
    "# get number of labels, ignore -100 index\n",
    "l = list(np.unique(labels))\n",
    "if -100 in l:\n",
    "    l.remove(-100)\n",
    "NUM_CLASSES = len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 45.709938049316406\n"
     ]
    }
   ],
   "source": [
    "enc = SeqEncoder(output_dim, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "dec = SeqDecoder(output_dim, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "dist = vMF(latent_dim, kappa=kap)\n",
    "model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1289.86, Test Loss: 5268.83,                   Time elapsed [s]: 30.63\n",
      "Epoch 1, Train Loss: 1061.45, Test Loss: 4860.99,                   Time elapsed [s]: 30.08\n",
      "Epoch 2, Train Loss: 828.44, Test Loss: 4949.02,                   Time elapsed [s]: 27.68\n",
      "Epoch 3, Train Loss: 722.19, Test Loss: 11748.03,                   Time elapsed [s]: 28.83\n",
      "Epoch 4, Train Loss: 657.73, Test Loss: 5002.20,                   Time elapsed [s]: 28.17\n",
      "Epoch 5, Train Loss: 608.50, Test Loss: 8761.36,                   Time elapsed [s]: 27.72\n",
      "Epoch 6, Train Loss: 588.02, Test Loss: 4315.08,                   Time elapsed [s]: 27.81\n",
      "Epoch 7, Train Loss: 558.37, Test Loss: 8646.86,                   Time elapsed [s]: 29.04\n",
      "Epoch 8, Train Loss: 531.87, Test Loss: 5249.23,                   Time elapsed [s]: 28.55\n",
      "Epoch 9, Train Loss: 528.03, Test Loss: 4985.69,                   Time elapsed [s]: 27.86\n",
      "Epoch 10, Train Loss: 519.47, Test Loss: 4488.93,                   Time elapsed [s]: 28.03\n",
      "Epoch 11, Train Loss: 508.07, Test Loss: 6859.74,                   Time elapsed [s]: 28.32\n",
      "Epoch 12, Train Loss: 494.22, Test Loss: 3212.25,                   Time elapsed [s]: 28.10\n",
      "Epoch 13, Train Loss: 491.34, Test Loss: 3553.50,                   Time elapsed [s]: 27.22\n",
      "Epoch 14, Train Loss: 483.76, Test Loss: 2819.27,                   Time elapsed [s]: 27.61\n",
      "Epoch 15, Train Loss: 480.04, Test Loss: 2912.95,                   Time elapsed [s]: 27.36\n",
      "Epoch 16, Train Loss: 478.17, Test Loss: 7158.13,                   Time elapsed [s]: 27.86\n",
      "Epoch 17, Train Loss: 476.56, Test Loss: 2829.43,                   Time elapsed [s]: 28.26\n",
      "Epoch 18, Train Loss: 471.31, Test Loss: 5997.46,                   Time elapsed [s]: 27.70\n",
      "Epoch 19, Train Loss: 478.68, Test Loss: 4753.98,                   Time elapsed [s]: 28.02\n",
      "Epoch 20, Train Loss: 466.66, Test Loss: 2606.91,                   Time elapsed [s]: 27.62\n",
      "Epoch 21, Train Loss: 456.80, Test Loss: 3046.32,                   Time elapsed [s]: 27.46\n",
      "Epoch 22, Train Loss: 447.95, Test Loss: 2452.28,                   Time elapsed [s]: 27.95\n",
      "Epoch 23, Train Loss: 439.17, Test Loss: 2817.18,                   Time elapsed [s]: 27.39\n",
      "Epoch 24, Train Loss: 437.58, Test Loss: 2696.16,                   Time elapsed [s]: 27.69\n",
      "Epoch 25, Train Loss: 469.69, Test Loss: 5945.63,                   Time elapsed [s]: 27.92\n",
      "Epoch 26, Train Loss: 450.30, Test Loss: 3009.50,                   Time elapsed [s]: 27.31\n",
      "Epoch 27, Train Loss: 421.83, Test Loss: 2269.77,                   Time elapsed [s]: 27.28\n",
      "Epoch 28, Train Loss: 412.88, Test Loss: 2379.95,                   Time elapsed [s]: 27.36\n",
      "Epoch 29, Train Loss: 419.94, Test Loss: 2824.95,                   Time elapsed [s]: 27.32\n",
      "Epoch 30, Train Loss: 451.95, Test Loss: 2619.82,                   Time elapsed [s]: 27.51\n",
      "Epoch 31, Train Loss: 428.37, Test Loss: 2382.28,                   Time elapsed [s]: 27.80\n",
      "Epoch 32, Train Loss: 445.76, Test Loss: 2055.02,                   Time elapsed [s]: 27.41\n",
      "Epoch 33, Train Loss: 418.79, Test Loss: 2241.82,                   Time elapsed [s]: 27.21\n",
      "Epoch 34, Train Loss: 402.88, Test Loss: 2005.46,                   Time elapsed [s]: 27.87\n",
      "Epoch 35, Train Loss: 396.91, Test Loss: 2032.76,                   Time elapsed [s]: 28.63\n",
      "Epoch 36, Train Loss: 407.27, Test Loss: 2048.84,                   Time elapsed [s]: 27.94\n",
      "Epoch 37, Train Loss: 387.95, Test Loss: 1936.90,                   Time elapsed [s]: 28.28\n",
      "Epoch 38, Train Loss: 372.94, Test Loss: 2172.14,                   Time elapsed [s]: 28.44\n",
      "Epoch 39, Train Loss: 373.31, Test Loss: 2028.71,                   Time elapsed [s]: 27.67\n",
      "Epoch 40, Train Loss: 372.08, Test Loss: 2190.33,                   Time elapsed [s]: 27.21\n",
      "Epoch 41, Train Loss: 369.90, Test Loss: 1850.89,                   Time elapsed [s]: 27.27\n",
      "Epoch 42, Train Loss: 362.06, Test Loss: 1963.02,                   Time elapsed [s]: 27.73\n",
      "Epoch 43, Train Loss: 385.50, Test Loss: 1771.60,                   Time elapsed [s]: 27.35\n",
      "Epoch 44, Train Loss: 364.46, Test Loss: 1829.76,                   Time elapsed [s]: 28.52\n",
      "Epoch 45, Train Loss: 361.35, Test Loss: 1630.17,                   Time elapsed [s]: 27.63\n",
      "Epoch 46, Train Loss: 347.31, Test Loss: 1614.58,                   Time elapsed [s]: 27.42\n",
      "Epoch 47, Train Loss: 341.94, Test Loss: 1752.96,                   Time elapsed [s]: 27.58\n",
      "Epoch 48, Train Loss: 365.21, Test Loss: 1615.81,                   Time elapsed [s]: 27.32\n",
      "Epoch 49, Train Loss: 357.16, Test Loss: 1708.64,                   Time elapsed [s]: 27.76\n",
      "Epoch 50, Train Loss: 346.02, Test Loss: 1578.71,                   Time elapsed [s]: 27.92\n",
      "Epoch 51, Train Loss: 322.39, Test Loss: 1489.63,                   Time elapsed [s]: 27.45\n",
      "Epoch 52, Train Loss: 319.17, Test Loss: 1516.91,                   Time elapsed [s]: 27.63\n",
      "Epoch 53, Train Loss: 311.40, Test Loss: 1624.98,                   Time elapsed [s]: 28.58\n",
      "Epoch 54, Train Loss: 323.65, Test Loss: 1545.13,                   Time elapsed [s]: 29.65\n",
      "Epoch 55, Train Loss: 320.24, Test Loss: 1487.08,                   Time elapsed [s]: 28.19\n",
      "Epoch 56, Train Loss: 306.24, Test Loss: 1496.98,                   Time elapsed [s]: 28.30\n",
      "Epoch 57, Train Loss: 313.16, Test Loss: 1417.28,                   Time elapsed [s]: 28.10\n",
      "Epoch 58, Train Loss: 304.84, Test Loss: 1551.19,                   Time elapsed [s]: 27.52\n",
      "Epoch 59, Train Loss: 306.38, Test Loss: 1389.96,                   Time elapsed [s]: 27.58\n",
      "Epoch 60, Train Loss: 307.33, Test Loss: 1320.14,                   Time elapsed [s]: 27.53\n",
      "Epoch 61, Train Loss: 297.51, Test Loss: 1320.43,                   Time elapsed [s]: 27.31\n",
      "Epoch 62, Train Loss: 291.70, Test Loss: 1351.94,                   Time elapsed [s]: 27.28\n",
      "Epoch 63, Train Loss: 293.04, Test Loss: 1288.50,                   Time elapsed [s]: 27.99\n",
      "Epoch 64, Train Loss: 292.12, Test Loss: 1276.67,                   Time elapsed [s]: 27.91\n",
      "Epoch 65, Train Loss: 283.34, Test Loss: 1280.23,                   Time elapsed [s]: 27.51\n",
      "Epoch 66, Train Loss: 288.49, Test Loss: 1270.72,                   Time elapsed [s]: 27.37\n",
      "Epoch 67, Train Loss: 278.95, Test Loss: 1245.52,                   Time elapsed [s]: 27.29\n",
      "Epoch 68, Train Loss: 274.54, Test Loss: 1242.02,                   Time elapsed [s]: 27.40\n",
      "Epoch 69, Train Loss: 278.85, Test Loss: 1233.22,                   Time elapsed [s]: 27.45\n",
      "Epoch 70, Train Loss: 271.24, Test Loss: 1302.06,                   Time elapsed [s]: 27.30\n",
      "Epoch 71, Train Loss: 264.98, Test Loss: 1459.55,                   Time elapsed [s]: 27.37\n",
      "Epoch 72, Train Loss: 267.26, Test Loss: 1330.18,                   Time elapsed [s]: 27.67\n",
      "Epoch 73, Train Loss: 279.18, Test Loss: 1224.68,                   Time elapsed [s]: 28.08\n",
      "Epoch 74, Train Loss: 282.45, Test Loss: 1210.52,                   Time elapsed [s]: 27.90\n",
      "Epoch 75, Train Loss: 262.80, Test Loss: 1263.72,                   Time elapsed [s]: 27.74\n",
      "Epoch 76, Train Loss: 260.60, Test Loss: 1310.15,                   Time elapsed [s]: 28.07\n",
      "Epoch 77, Train Loss: 270.01, Test Loss: 1205.64,                   Time elapsed [s]: 27.67\n",
      "Epoch 78, Train Loss: 254.89, Test Loss: 1321.05,                   Time elapsed [s]: 28.02\n",
      "Epoch 79, Train Loss: 278.07, Test Loss: 1239.12,                   Time elapsed [s]: 28.77\n",
      "Epoch 80, Train Loss: 276.91, Test Loss: 1145.44,                   Time elapsed [s]: 27.25\n",
      "Epoch 81, Train Loss: 256.98, Test Loss: 1105.24,                   Time elapsed [s]: 27.26\n",
      "Epoch 82, Train Loss: 255.42, Test Loss: 1158.09,                   Time elapsed [s]: 27.24\n",
      "Epoch 83, Train Loss: 266.40, Test Loss: 1185.72,                   Time elapsed [s]: 27.26\n",
      "Epoch 84, Train Loss: 273.15, Test Loss: 1182.37,                   Time elapsed [s]: 27.93\n",
      "Epoch 85, Train Loss: 273.15, Test Loss: 1376.62,                   Time elapsed [s]: 28.19\n",
      "Epoch 86, Train Loss: 289.95, Test Loss: 1220.30,                   Time elapsed [s]: 27.53\n",
      "Epoch 87, Train Loss: 270.16, Test Loss: 1202.86,                   Time elapsed [s]: 27.28\n",
      "Epoch 88, Train Loss: 268.43, Test Loss: 1207.73,                   Time elapsed [s]: 27.24\n",
      "Epoch 89, Train Loss: 256.46, Test Loss: 1056.43,                   Time elapsed [s]: 27.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Train Loss: 263.02, Test Loss: 1240.63,                   Time elapsed [s]: 27.99\n",
      "Epoch 91, Train Loss: 280.45, Test Loss: 1475.08,                   Time elapsed [s]: 27.56\n",
      "Epoch 92, Train Loss: 276.22, Test Loss: 1285.06,                   Time elapsed [s]: 27.63\n",
      "Epoch 93, Train Loss: 297.65, Test Loss: 1364.56,                   Time elapsed [s]: 27.52\n",
      "Epoch 94, Train Loss: 284.62, Test Loss: 1204.52,                   Time elapsed [s]: 27.54\n",
      "Epoch 95, Train Loss: 310.67, Test Loss: 1583.70,                   Time elapsed [s]: 27.52\n",
      "Epoch 96, Train Loss: 283.63, Test Loss: 1235.13,                   Time elapsed [s]: 27.34\n",
      "Epoch 97, Train Loss: 265.79, Test Loss: 1267.63,                   Time elapsed [s]: 27.88\n",
      "Epoch 98, Train Loss: 280.46, Test Loss: 1272.72,                   Time elapsed [s]: 28.30\n",
      "Epoch 99, Train Loss: 265.32, Test Loss: 1133.58,                   Time elapsed [s]: 28.00\n",
      "Epoch 100, Train Loss: 258.79, Test Loss: 1075.67,                   Time elapsed [s]: 27.43\n",
      "Epoch 101, Train Loss: 241.30, Test Loss: 978.94,                   Time elapsed [s]: 28.29\n",
      "Epoch 102, Train Loss: 233.94, Test Loss: 938.26,                   Time elapsed [s]: 27.68\n",
      "Epoch 103, Train Loss: 230.61, Test Loss: 976.08,                   Time elapsed [s]: 27.33\n",
      "Epoch 104, Train Loss: 230.67, Test Loss: 1031.97,                   Time elapsed [s]: 27.46\n",
      "Epoch 105, Train Loss: 230.35, Test Loss: 968.67,                   Time elapsed [s]: 27.48\n",
      "Epoch 106, Train Loss: 226.99, Test Loss: 937.68,                   Time elapsed [s]: 28.04\n",
      "Epoch 107, Train Loss: 225.67, Test Loss: 927.84,                   Time elapsed [s]: 27.81\n",
      "Epoch 108, Train Loss: 226.67, Test Loss: 891.36,                   Time elapsed [s]: 27.69\n",
      "Epoch 109, Train Loss: 220.42, Test Loss: 889.26,                   Time elapsed [s]: 27.34\n",
      "Epoch 110, Train Loss: 221.04, Test Loss: 893.34,                   Time elapsed [s]: 27.70\n",
      "Epoch 111, Train Loss: 224.50, Test Loss: 925.00,                   Time elapsed [s]: 27.35\n",
      "Epoch 112, Train Loss: 217.98, Test Loss: 930.61,                   Time elapsed [s]: 27.30\n",
      "Epoch 113, Train Loss: 217.36, Test Loss: 941.52,                   Time elapsed [s]: 27.44\n",
      "Epoch 114, Train Loss: 222.80, Test Loss: 915.28,                   Time elapsed [s]: 27.46\n",
      "Epoch 115, Train Loss: 217.39, Test Loss: 973.42,                   Time elapsed [s]: 27.53\n",
      "Epoch 116, Train Loss: 219.55, Test Loss: 939.80,                   Time elapsed [s]: 27.25\n",
      "Epoch 117, Train Loss: 221.11, Test Loss: 928.04,                   Time elapsed [s]: 27.50\n",
      "Epoch 118, Train Loss: 218.66, Test Loss: 863.85,                   Time elapsed [s]: 27.61\n",
      "Epoch 119, Train Loss: 214.11, Test Loss: 868.88,                   Time elapsed [s]: 28.59\n",
      "Epoch 120, Train Loss: 210.09, Test Loss: 895.84,                   Time elapsed [s]: 27.45\n",
      "Epoch 121, Train Loss: 211.47, Test Loss: 862.02,                   Time elapsed [s]: 27.70\n",
      "Epoch 122, Train Loss: 217.04, Test Loss: 851.73,                   Time elapsed [s]: 27.63\n",
      "Epoch 123, Train Loss: 224.50, Test Loss: 866.01,                   Time elapsed [s]: 28.14\n",
      "Epoch 124, Train Loss: 212.48, Test Loss: 858.32,                   Time elapsed [s]: 27.58\n",
      "Epoch 125, Train Loss: 210.18, Test Loss: 885.62,                   Time elapsed [s]: 27.89\n",
      "Epoch 126, Train Loss: 209.94, Test Loss: 883.01,                   Time elapsed [s]: 27.26\n",
      "Epoch 127, Train Loss: 216.76, Test Loss: 868.24,                   Time elapsed [s]: 27.29\n",
      "Epoch 128, Train Loss: 214.89, Test Loss: 946.27,                   Time elapsed [s]: 27.06\n",
      "Epoch 129, Train Loss: 216.68, Test Loss: 830.28,                   Time elapsed [s]: 27.25\n",
      "Epoch 130, Train Loss: 208.67, Test Loss: 831.73,                   Time elapsed [s]: 27.06\n",
      "Epoch 131, Train Loss: 206.04, Test Loss: 845.10,                   Time elapsed [s]: 27.08\n",
      "Epoch 132, Train Loss: 207.73, Test Loss: 825.32,                   Time elapsed [s]: 27.42\n",
      "Epoch 133, Train Loss: 205.10, Test Loss: 849.50,                   Time elapsed [s]: 27.69\n",
      "Epoch 134, Train Loss: 206.33, Test Loss: 840.78,                   Time elapsed [s]: 27.68\n",
      "Epoch 135, Train Loss: 204.42, Test Loss: 875.31,                   Time elapsed [s]: 27.20\n",
      "Epoch 136, Train Loss: 204.68, Test Loss: 909.22,                   Time elapsed [s]: 27.12\n",
      "Epoch 137, Train Loss: 203.41, Test Loss: 881.15,                   Time elapsed [s]: 27.09\n",
      "Epoch 138, Train Loss: 203.63, Test Loss: 815.40,                   Time elapsed [s]: 27.14\n",
      "Epoch 139, Train Loss: 208.16, Test Loss: 799.55,                   Time elapsed [s]: 27.52\n",
      "Epoch 140, Train Loss: 201.73, Test Loss: 817.80,                   Time elapsed [s]: 27.20\n",
      "Epoch 141, Train Loss: 201.58, Test Loss: 860.81,                   Time elapsed [s]: 27.23\n",
      "Epoch 142, Train Loss: 204.56, Test Loss: 894.36,                   Time elapsed [s]: 27.16\n",
      "Epoch 143, Train Loss: 207.31, Test Loss: 856.76,                   Time elapsed [s]: 27.31\n",
      "Epoch 144, Train Loss: 213.12, Test Loss: 823.62,                   Time elapsed [s]: 27.45\n",
      "Epoch 145, Train Loss: 211.71, Test Loss: 854.26,                   Time elapsed [s]: 27.20\n",
      "Epoch 146, Train Loss: 205.66, Test Loss: 817.45,                   Time elapsed [s]: 27.37\n",
      "Epoch 147, Train Loss: 213.11, Test Loss: 803.10,                   Time elapsed [s]: 27.35\n",
      "Epoch 148, Train Loss: 206.78, Test Loss: 826.59,                   Time elapsed [s]: 27.12\n",
      "Epoch 149, Train Loss: 200.07, Test Loss: 798.22,                   Time elapsed [s]: 27.25\n",
      "Epoch 0, Train Loss: 1287.89, Test Loss: 7134.06,                   Time elapsed [s]: 28.04\n",
      "Epoch 1, Train Loss: 943.35, Test Loss: 3698.12,                   Time elapsed [s]: 28.16\n",
      "Epoch 2, Train Loss: 783.10, Test Loss: 5291.84,                   Time elapsed [s]: 27.19\n",
      "Epoch 3, Train Loss: 667.52, Test Loss: 3246.84,                   Time elapsed [s]: 27.23\n",
      "Epoch 4, Train Loss: 610.75, Test Loss: 16428.28,                   Time elapsed [s]: 27.36\n",
      "Epoch 5, Train Loss: 581.58, Test Loss: 3447.04,                   Time elapsed [s]: 27.34\n",
      "Epoch 6, Train Loss: 563.15, Test Loss: 7167.45,                   Time elapsed [s]: 27.15\n",
      "Epoch 7, Train Loss: 539.65, Test Loss: 3579.61,                   Time elapsed [s]: 27.20\n",
      "Epoch 8, Train Loss: 523.87, Test Loss: 9840.60,                   Time elapsed [s]: 27.52\n",
      "Epoch 9, Train Loss: 516.06, Test Loss: 7069.43,                   Time elapsed [s]: 27.16\n",
      "Epoch 10, Train Loss: 509.74, Test Loss: 4069.28,                   Time elapsed [s]: 27.10\n",
      "Epoch 11, Train Loss: 498.21, Test Loss: 2923.65,                   Time elapsed [s]: 27.48\n",
      "Epoch 12, Train Loss: 494.89, Test Loss: 3352.91,                   Time elapsed [s]: 27.88\n",
      "Epoch 13, Train Loss: 489.16, Test Loss: 2851.19,                   Time elapsed [s]: 27.55\n",
      "Epoch 14, Train Loss: 487.90, Test Loss: 4641.22,                   Time elapsed [s]: 27.14\n",
      "Epoch 15, Train Loss: 494.30, Test Loss: 3007.74,                   Time elapsed [s]: 27.13\n",
      "Epoch 16, Train Loss: 484.24, Test Loss: 5546.64,                   Time elapsed [s]: 27.32\n",
      "Epoch 17, Train Loss: 479.46, Test Loss: 3048.14,                   Time elapsed [s]: 27.31\n",
      "Epoch 18, Train Loss: 478.30, Test Loss: 2904.98,                   Time elapsed [s]: 27.14\n",
      "Epoch 19, Train Loss: 469.39, Test Loss: 3188.07,                   Time elapsed [s]: 27.03\n",
      "Epoch 20, Train Loss: 462.28, Test Loss: 3377.77,                   Time elapsed [s]: 27.03\n",
      "Epoch 21, Train Loss: 463.74, Test Loss: 2432.68,                   Time elapsed [s]: 27.17\n",
      "Epoch 22, Train Loss: 450.37, Test Loss: 2466.82,                   Time elapsed [s]: 27.19\n",
      "Epoch 23, Train Loss: 449.34, Test Loss: 2625.13,                   Time elapsed [s]: 27.02\n",
      "Epoch 24, Train Loss: 442.75, Test Loss: 2421.11,                   Time elapsed [s]: 26.97\n",
      "Epoch 25, Train Loss: 440.01, Test Loss: 2191.23,                   Time elapsed [s]: 27.08\n",
      "Epoch 26, Train Loss: 427.23, Test Loss: 2189.23,                   Time elapsed [s]: 27.66\n",
      "Epoch 27, Train Loss: 421.64, Test Loss: 2224.70,                   Time elapsed [s]: 27.52\n",
      "Epoch 28, Train Loss: 432.33, Test Loss: 2195.78,                   Time elapsed [s]: 27.33\n",
      "Epoch 29, Train Loss: 420.63, Test Loss: 2059.61,                   Time elapsed [s]: 27.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Train Loss: 416.37, Test Loss: 2182.83,                   Time elapsed [s]: 27.27\n",
      "Epoch 31, Train Loss: 430.28, Test Loss: 3079.47,                   Time elapsed [s]: 27.24\n",
      "Epoch 32, Train Loss: 435.13, Test Loss: 2912.06,                   Time elapsed [s]: 26.98\n",
      "Epoch 33, Train Loss: 428.57, Test Loss: 1999.74,                   Time elapsed [s]: 27.10\n",
      "Epoch 34, Train Loss: 409.95, Test Loss: 1942.98,                   Time elapsed [s]: 27.25\n",
      "Epoch 35, Train Loss: 397.54, Test Loss: 2175.72,                   Time elapsed [s]: 27.78\n",
      "Epoch 36, Train Loss: 381.75, Test Loss: 1849.08,                   Time elapsed [s]: 27.65\n",
      "Epoch 37, Train Loss: 385.92, Test Loss: 1686.44,                   Time elapsed [s]: 27.97\n",
      "Epoch 38, Train Loss: 383.58, Test Loss: 1757.14,                   Time elapsed [s]: 28.07\n",
      "Epoch 39, Train Loss: 385.67, Test Loss: 2567.98,                   Time elapsed [s]: 27.07\n",
      "Epoch 40, Train Loss: 392.90, Test Loss: 2836.08,                   Time elapsed [s]: 27.08\n",
      "Epoch 41, Train Loss: 406.34, Test Loss: 1661.94,                   Time elapsed [s]: 26.96\n",
      "Epoch 42, Train Loss: 401.57, Test Loss: 1882.99,                   Time elapsed [s]: 27.54\n",
      "Epoch 43, Train Loss: 385.36, Test Loss: 2423.52,                   Time elapsed [s]: 27.33\n",
      "Epoch 44, Train Loss: 375.95, Test Loss: 1940.50,                   Time elapsed [s]: 27.32\n",
      "Epoch 45, Train Loss: 381.94, Test Loss: 1701.10,                   Time elapsed [s]: 27.32\n",
      "Epoch 46, Train Loss: 349.14, Test Loss: 1999.84,                   Time elapsed [s]: 27.40\n",
      "Epoch 47, Train Loss: 365.06, Test Loss: 1978.20,                   Time elapsed [s]: 27.07\n",
      "Epoch 48, Train Loss: 358.06, Test Loss: 1522.62,                   Time elapsed [s]: 27.09\n",
      "Epoch 49, Train Loss: 347.99, Test Loss: 1486.23,                   Time elapsed [s]: 27.06\n",
      "Epoch 50, Train Loss: 336.16, Test Loss: 1632.52,                   Time elapsed [s]: 27.14\n",
      "Epoch 51, Train Loss: 319.45, Test Loss: 1329.73,                   Time elapsed [s]: 27.21\n",
      "Epoch 52, Train Loss: 318.37, Test Loss: 1399.70,                   Time elapsed [s]: 27.09\n",
      "Epoch 53, Train Loss: 301.10, Test Loss: 1314.59,                   Time elapsed [s]: 27.22\n",
      "Epoch 54, Train Loss: 300.69, Test Loss: 1293.06,                   Time elapsed [s]: 27.41\n",
      "Epoch 55, Train Loss: 291.73, Test Loss: 1227.88,                   Time elapsed [s]: 27.08\n",
      "Epoch 56, Train Loss: 293.98, Test Loss: 1215.65,                   Time elapsed [s]: 26.98\n",
      "Epoch 57, Train Loss: 285.40, Test Loss: 1273.29,                   Time elapsed [s]: 27.16\n",
      "Epoch 58, Train Loss: 284.50, Test Loss: 1339.93,                   Time elapsed [s]: 27.01\n",
      "Epoch 59, Train Loss: 296.19, Test Loss: 1284.17,                   Time elapsed [s]: 27.24\n",
      "Epoch 60, Train Loss: 291.48, Test Loss: 1170.49,                   Time elapsed [s]: 27.19\n",
      "Epoch 61, Train Loss: 281.35, Test Loss: 1216.61,                   Time elapsed [s]: 27.20\n",
      "Epoch 62, Train Loss: 284.24, Test Loss: 1234.00,                   Time elapsed [s]: 27.16\n",
      "Epoch 63, Train Loss: 280.99, Test Loss: 1116.41,                   Time elapsed [s]: 27.61\n",
      "Epoch 64, Train Loss: 282.22, Test Loss: 1242.15,                   Time elapsed [s]: 27.76\n",
      "Epoch 65, Train Loss: 281.33, Test Loss: 1301.94,                   Time elapsed [s]: 28.23\n",
      "Epoch 66, Train Loss: 287.66, Test Loss: 1337.73,                   Time elapsed [s]: 27.17\n",
      "Epoch 67, Train Loss: 296.26, Test Loss: 1272.86,                   Time elapsed [s]: 27.32\n",
      "Epoch 68, Train Loss: 293.16, Test Loss: 1238.41,                   Time elapsed [s]: 26.88\n",
      "Epoch 69, Train Loss: 276.76, Test Loss: 1135.30,                   Time elapsed [s]: 27.03\n",
      "Epoch 70, Train Loss: 273.51, Test Loss: 1206.74,                   Time elapsed [s]: 27.11\n",
      "Epoch 71, Train Loss: 294.13, Test Loss: 1324.23,                   Time elapsed [s]: 27.06\n",
      "Epoch 72, Train Loss: 282.37, Test Loss: 1439.94,                   Time elapsed [s]: 27.31\n",
      "Epoch 73, Train Loss: 284.08, Test Loss: 1230.72,                   Time elapsed [s]: 27.39\n",
      "Epoch 74, Train Loss: 287.26, Test Loss: 1256.54,                   Time elapsed [s]: 28.02\n",
      "Epoch 75, Train Loss: 273.29, Test Loss: 1168.48,                   Time elapsed [s]: 27.54\n",
      "Epoch 76, Train Loss: 284.49, Test Loss: 1111.31,                   Time elapsed [s]: 27.01\n",
      "Epoch 77, Train Loss: 266.37, Test Loss: 1123.08,                   Time elapsed [s]: 27.42\n",
      "Epoch 78, Train Loss: 265.85, Test Loss: 1187.70,                   Time elapsed [s]: 27.43\n",
      "Epoch 79, Train Loss: 261.00, Test Loss: 1294.92,                   Time elapsed [s]: 27.29\n",
      "Epoch 80, Train Loss: 270.28, Test Loss: 1102.61,                   Time elapsed [s]: 27.26\n",
      "Epoch 81, Train Loss: 292.95, Test Loss: 1446.61,                   Time elapsed [s]: 27.51\n",
      "Epoch 82, Train Loss: 305.92, Test Loss: 1457.81,                   Time elapsed [s]: 27.53\n",
      "Epoch 83, Train Loss: 311.16, Test Loss: 1375.35,                   Time elapsed [s]: 27.08\n",
      "Epoch 84, Train Loss: 290.27, Test Loss: 1138.62,                   Time elapsed [s]: 27.10\n",
      "Epoch 85, Train Loss: 274.18, Test Loss: 1066.06,                   Time elapsed [s]: 27.20\n",
      "Epoch 86, Train Loss: 269.68, Test Loss: 1095.91,                   Time elapsed [s]: 27.16\n",
      "Epoch 87, Train Loss: 257.76, Test Loss: 1234.91,                   Time elapsed [s]: 27.16\n",
      "Epoch 88, Train Loss: 268.29, Test Loss: 1079.40,                   Time elapsed [s]: 27.25\n",
      "Epoch 89, Train Loss: 272.45, Test Loss: 1123.20,                   Time elapsed [s]: 27.25\n",
      "Epoch 90, Train Loss: 266.09, Test Loss: 1124.54,                   Time elapsed [s]: 27.45\n",
      "Epoch 91, Train Loss: 255.81, Test Loss: 1019.51,                   Time elapsed [s]: 27.15\n",
      "Epoch 92, Train Loss: 245.85, Test Loss: 1022.91,                   Time elapsed [s]: 27.18\n",
      "Epoch 93, Train Loss: 263.97, Test Loss: 1031.84,                   Time elapsed [s]: 27.22\n",
      "Epoch 94, Train Loss: 259.31, Test Loss: 996.60,                   Time elapsed [s]: 27.18\n",
      "Epoch 95, Train Loss: 246.96, Test Loss: 1168.93,                   Time elapsed [s]: 27.23\n",
      "Epoch 96, Train Loss: 265.10, Test Loss: 1079.34,                   Time elapsed [s]: 28.79\n",
      "Epoch 97, Train Loss: 261.53, Test Loss: 1113.89,                   Time elapsed [s]: 27.87\n",
      "Epoch 98, Train Loss: 265.69, Test Loss: 1182.08,                   Time elapsed [s]: 27.57\n",
      "Epoch 99, Train Loss: 265.45, Test Loss: 1206.17,                   Time elapsed [s]: 27.73\n",
      "Epoch 100, Train Loss: 253.45, Test Loss: 1007.85,                   Time elapsed [s]: 27.43\n",
      "Epoch 101, Train Loss: 243.77, Test Loss: 997.06,                   Time elapsed [s]: 27.38\n",
      "Epoch 102, Train Loss: 239.96, Test Loss: 957.65,                   Time elapsed [s]: 27.03\n",
      "Epoch 103, Train Loss: 232.38, Test Loss: 990.30,                   Time elapsed [s]: 27.63\n",
      "Epoch 104, Train Loss: 238.85, Test Loss: 946.99,                   Time elapsed [s]: 27.16\n",
      "Epoch 105, Train Loss: 235.79, Test Loss: 1015.65,                   Time elapsed [s]: 27.44\n",
      "Epoch 106, Train Loss: 233.34, Test Loss: 987.69,                   Time elapsed [s]: 27.03\n",
      "Epoch 107, Train Loss: 230.20, Test Loss: 932.81,                   Time elapsed [s]: 27.05\n",
      "Epoch 108, Train Loss: 234.63, Test Loss: 979.19,                   Time elapsed [s]: 27.23\n",
      "Epoch 109, Train Loss: 237.33, Test Loss: 944.36,                   Time elapsed [s]: 27.17\n",
      "Epoch 110, Train Loss: 231.54, Test Loss: 914.68,                   Time elapsed [s]: 27.06\n",
      "Epoch 111, Train Loss: 230.35, Test Loss: 909.62,                   Time elapsed [s]: 27.62\n",
      "Epoch 112, Train Loss: 225.35, Test Loss: 883.65,                   Time elapsed [s]: 27.15\n",
      "Epoch 113, Train Loss: 222.74, Test Loss: 896.71,                   Time elapsed [s]: 27.06\n",
      "Epoch 114, Train Loss: 223.09, Test Loss: 912.06,                   Time elapsed [s]: 27.19\n",
      "Epoch 115, Train Loss: 227.13, Test Loss: 1019.16,                   Time elapsed [s]: 27.77\n",
      "Epoch 116, Train Loss: 227.64, Test Loss: 895.69,                   Time elapsed [s]: 28.10\n",
      "Epoch 117, Train Loss: 227.64, Test Loss: 905.41,                   Time elapsed [s]: 27.43\n",
      "Epoch 118, Train Loss: 222.78, Test Loss: 884.99,                   Time elapsed [s]: 27.27\n",
      "Epoch 119, Train Loss: 219.60, Test Loss: 860.67,                   Time elapsed [s]: 27.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120, Train Loss: 221.67, Test Loss: 856.06,                   Time elapsed [s]: 27.35\n",
      "Epoch 121, Train Loss: 217.48, Test Loss: 863.32,                   Time elapsed [s]: 27.13\n",
      "Epoch 122, Train Loss: 216.66, Test Loss: 887.56,                   Time elapsed [s]: 27.18\n",
      "Epoch 123, Train Loss: 221.92, Test Loss: 849.75,                   Time elapsed [s]: 27.10\n",
      "Epoch 124, Train Loss: 220.02, Test Loss: 871.14,                   Time elapsed [s]: 27.19\n",
      "Epoch 125, Train Loss: 217.90, Test Loss: 871.45,                   Time elapsed [s]: 27.61\n",
      "Epoch 126, Train Loss: 213.34, Test Loss: 885.24,                   Time elapsed [s]: 27.38\n",
      "Epoch 127, Train Loss: 214.37, Test Loss: 878.56,                   Time elapsed [s]: 27.25\n",
      "Epoch 128, Train Loss: 219.75, Test Loss: 885.41,                   Time elapsed [s]: 27.20\n",
      "Epoch 129, Train Loss: 223.17, Test Loss: 891.54,                   Time elapsed [s]: 27.26\n",
      "Epoch 130, Train Loss: 217.59, Test Loss: 835.55,                   Time elapsed [s]: 27.60\n",
      "Epoch 131, Train Loss: 222.66, Test Loss: 894.69,                   Time elapsed [s]: 27.30\n",
      "Epoch 132, Train Loss: 219.10, Test Loss: 936.13,                   Time elapsed [s]: 27.36\n",
      "Epoch 133, Train Loss: 217.77, Test Loss: 877.24,                   Time elapsed [s]: 27.70\n",
      "Epoch 134, Train Loss: 216.69, Test Loss: 886.65,                   Time elapsed [s]: 27.35\n",
      "Epoch 135, Train Loss: 220.56, Test Loss: 889.83,                   Time elapsed [s]: 27.32\n",
      "Epoch 136, Train Loss: 214.03, Test Loss: 854.74,                   Time elapsed [s]: 26.99\n",
      "Epoch 137, Train Loss: 209.33, Test Loss: 855.25,                   Time elapsed [s]: 27.03\n",
      "Epoch 138, Train Loss: 210.15, Test Loss: 877.58,                   Time elapsed [s]: 27.03\n",
      "Epoch 139, Train Loss: 216.25, Test Loss: 824.64,                   Time elapsed [s]: 27.15\n",
      "Epoch 140, Train Loss: 217.00, Test Loss: 848.75,                   Time elapsed [s]: 27.24\n",
      "Epoch 141, Train Loss: 208.35, Test Loss: 833.92,                   Time elapsed [s]: 27.12\n",
      "Epoch 142, Train Loss: 214.32, Test Loss: 852.09,                   Time elapsed [s]: 27.42\n",
      "Epoch 143, Train Loss: 213.11, Test Loss: 865.00,                   Time elapsed [s]: 27.19\n",
      "Epoch 144, Train Loss: 213.28, Test Loss: 867.81,                   Time elapsed [s]: 27.25\n",
      "Epoch 145, Train Loss: 219.86, Test Loss: 798.84,                   Time elapsed [s]: 27.21\n",
      "Epoch 146, Train Loss: 212.16, Test Loss: 851.44,                   Time elapsed [s]: 27.38\n",
      "Epoch 147, Train Loss: 214.53, Test Loss: 814.40,                   Time elapsed [s]: 28.41\n",
      "Epoch 148, Train Loss: 211.76, Test Loss: 837.43,                   Time elapsed [s]: 27.26\n",
      "Epoch 149, Train Loss: 210.68, Test Loss: 834.49,                   Time elapsed [s]: 27.12\n",
      "Epoch 0, Train Loss: 1289.11, Test Loss: 7343.99,                   Time elapsed [s]: 27.26\n",
      "Epoch 1, Train Loss: 1039.56, Test Loss: 5435.55,                   Time elapsed [s]: 27.24\n",
      "Epoch 2, Train Loss: 870.61, Test Loss: 6902.75,                   Time elapsed [s]: 27.20\n",
      "Epoch 3, Train Loss: 783.60, Test Loss: 6736.19,                   Time elapsed [s]: 27.64\n",
      "Epoch 4, Train Loss: 673.18, Test Loss: 7458.72,                   Time elapsed [s]: 27.73\n",
      "Epoch 5, Train Loss: 604.18, Test Loss: 5954.71,                   Time elapsed [s]: 27.53\n",
      "Epoch 6, Train Loss: 568.05, Test Loss: 3397.05,                   Time elapsed [s]: 27.56\n",
      "Epoch 7, Train Loss: 549.64, Test Loss: 7119.10,                   Time elapsed [s]: 27.53\n",
      "Epoch 8, Train Loss: 534.15, Test Loss: 3865.82,                   Time elapsed [s]: 27.54\n",
      "Epoch 9, Train Loss: 518.04, Test Loss: 3226.63,                   Time elapsed [s]: 27.30\n",
      "Epoch 10, Train Loss: 503.23, Test Loss: 5379.05,                   Time elapsed [s]: 27.09\n",
      "Epoch 11, Train Loss: 494.34, Test Loss: 3307.68,                   Time elapsed [s]: 27.25\n",
      "Epoch 12, Train Loss: 488.85, Test Loss: 2930.88,                   Time elapsed [s]: 27.25\n",
      "Epoch 13, Train Loss: 491.72, Test Loss: 6201.97,                   Time elapsed [s]: 27.27\n",
      "Epoch 14, Train Loss: 485.99, Test Loss: 2789.17,                   Time elapsed [s]: 27.22\n",
      "Epoch 15, Train Loss: 478.91, Test Loss: 2701.73,                   Time elapsed [s]: 27.12\n",
      "Epoch 16, Train Loss: 473.71, Test Loss: 3173.36,                   Time elapsed [s]: 27.14\n",
      "Epoch 17, Train Loss: 466.39, Test Loss: 2587.19,                   Time elapsed [s]: 27.58\n",
      "Epoch 18, Train Loss: 464.18, Test Loss: 5598.21,                   Time elapsed [s]: 27.36\n",
      "Epoch 19, Train Loss: 458.20, Test Loss: 2376.07,                   Time elapsed [s]: 27.32\n",
      "Epoch 20, Train Loss: 457.43, Test Loss: 2263.39,                   Time elapsed [s]: 27.30\n",
      "Epoch 21, Train Loss: 453.39, Test Loss: 2522.92,                   Time elapsed [s]: 27.35\n",
      "Epoch 22, Train Loss: 446.18, Test Loss: 2223.44,                   Time elapsed [s]: 27.86\n",
      "Epoch 23, Train Loss: 452.64, Test Loss: 2492.75,                   Time elapsed [s]: 28.10\n",
      "Epoch 24, Train Loss: 426.77, Test Loss: 2225.43,                   Time elapsed [s]: 27.30\n",
      "Epoch 25, Train Loss: 429.70, Test Loss: 2380.76,                   Time elapsed [s]: 27.25\n",
      "Epoch 26, Train Loss: 423.23, Test Loss: 3066.69,                   Time elapsed [s]: 27.54\n",
      "Epoch 27, Train Loss: 420.69, Test Loss: 2230.47,                   Time elapsed [s]: 27.41\n",
      "Epoch 28, Train Loss: 412.41, Test Loss: 2196.35,                   Time elapsed [s]: 27.10\n",
      "Epoch 29, Train Loss: 414.71, Test Loss: 1932.40,                   Time elapsed [s]: 27.21\n",
      "Epoch 30, Train Loss: 409.72, Test Loss: 1930.01,                   Time elapsed [s]: 27.58\n",
      "Epoch 31, Train Loss: 404.01, Test Loss: 2035.97,                   Time elapsed [s]: 27.22\n",
      "Epoch 32, Train Loss: 398.80, Test Loss: 1975.12,                   Time elapsed [s]: 27.36\n",
      "Epoch 33, Train Loss: 404.15, Test Loss: 1883.52,                   Time elapsed [s]: 27.33\n",
      "Epoch 34, Train Loss: 394.85, Test Loss: 1998.09,                   Time elapsed [s]: 27.23\n",
      "Epoch 35, Train Loss: 378.99, Test Loss: 1828.63,                   Time elapsed [s]: 27.50\n",
      "Epoch 36, Train Loss: 390.83, Test Loss: 1905.35,                   Time elapsed [s]: 27.60\n",
      "Epoch 37, Train Loss: 382.61, Test Loss: 1859.87,                   Time elapsed [s]: 27.27\n",
      "Epoch 38, Train Loss: 388.13, Test Loss: 2019.43,                   Time elapsed [s]: 27.07\n",
      "Epoch 39, Train Loss: 417.61, Test Loss: 2114.08,                   Time elapsed [s]: 27.21\n",
      "Epoch 40, Train Loss: 401.50, Test Loss: 2712.50,                   Time elapsed [s]: 27.19\n",
      "Epoch 41, Train Loss: 383.01, Test Loss: 1980.67,                   Time elapsed [s]: 27.31\n",
      "Epoch 42, Train Loss: 370.54, Test Loss: 1931.06,                   Time elapsed [s]: 28.42\n",
      "Epoch 43, Train Loss: 396.81, Test Loss: 2011.74,                   Time elapsed [s]: 27.20\n",
      "Epoch 44, Train Loss: 374.90, Test Loss: 2280.73,                   Time elapsed [s]: 27.41\n",
      "Epoch 45, Train Loss: 373.67, Test Loss: 2103.72,                   Time elapsed [s]: 27.56\n",
      "Epoch 46, Train Loss: 394.60, Test Loss: 1783.70,                   Time elapsed [s]: 27.18\n",
      "Epoch 47, Train Loss: 376.39, Test Loss: 1641.25,                   Time elapsed [s]: 27.18\n",
      "Epoch 48, Train Loss: 356.73, Test Loss: 1739.05,                   Time elapsed [s]: 27.17\n",
      "Epoch 49, Train Loss: 346.31, Test Loss: 1776.49,                   Time elapsed [s]: 27.29\n",
      "Epoch 50, Train Loss: 369.87, Test Loss: 1734.09,                   Time elapsed [s]: 27.84\n",
      "Epoch 51, Train Loss: 344.51, Test Loss: 1942.70,                   Time elapsed [s]: 27.58\n",
      "Epoch 52, Train Loss: 331.63, Test Loss: 1655.15,                   Time elapsed [s]: 27.25\n",
      "Epoch 53, Train Loss: 336.48, Test Loss: 1649.53,                   Time elapsed [s]: 27.27\n",
      "Epoch 54, Train Loss: 322.53, Test Loss: 1439.83,                   Time elapsed [s]: 27.15\n",
      "Epoch 55, Train Loss: 307.41, Test Loss: 1495.46,                   Time elapsed [s]: 27.20\n",
      "Epoch 56, Train Loss: 306.92, Test Loss: 1406.73,                   Time elapsed [s]: 27.39\n",
      "Epoch 57, Train Loss: 303.87, Test Loss: 1560.77,                   Time elapsed [s]: 27.97\n",
      "Epoch 58, Train Loss: 309.22, Test Loss: 1510.98,                   Time elapsed [s]: 26.96\n",
      "Epoch 59, Train Loss: 307.73, Test Loss: 1513.17,                   Time elapsed [s]: 27.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Train Loss: 297.61, Test Loss: 1487.89,                   Time elapsed [s]: 27.36\n",
      "Epoch 61, Train Loss: 312.35, Test Loss: 1404.19,                   Time elapsed [s]: 27.08\n",
      "Epoch 62, Train Loss: 298.69, Test Loss: 1462.97,                   Time elapsed [s]: 27.17\n",
      "Epoch 63, Train Loss: 296.33, Test Loss: 1401.80,                   Time elapsed [s]: 27.14\n",
      "Epoch 64, Train Loss: 298.32, Test Loss: 1350.20,                   Time elapsed [s]: 27.11\n",
      "Epoch 65, Train Loss: 305.78, Test Loss: 1445.36,                   Time elapsed [s]: 27.48\n",
      "Epoch 66, Train Loss: 290.48, Test Loss: 1386.63,                   Time elapsed [s]: 27.52\n",
      "Epoch 67, Train Loss: 282.65, Test Loss: 1489.09,                   Time elapsed [s]: 28.35\n",
      "Epoch 68, Train Loss: 293.59, Test Loss: 1299.61,                   Time elapsed [s]: 27.83\n",
      "Epoch 69, Train Loss: 303.89, Test Loss: 1510.65,                   Time elapsed [s]: 27.36\n",
      "Epoch 70, Train Loss: 293.38, Test Loss: 1610.70,                   Time elapsed [s]: 27.15\n",
      "Epoch 71, Train Loss: 302.53, Test Loss: 1323.26,                   Time elapsed [s]: 27.50\n",
      "Epoch 72, Train Loss: 300.25, Test Loss: 1360.79,                   Time elapsed [s]: 28.93\n",
      "Epoch 73, Train Loss: 286.20, Test Loss: 1375.95,                   Time elapsed [s]: 27.24\n",
      "Epoch 74, Train Loss: 282.01, Test Loss: 1220.99,                   Time elapsed [s]: 28.19\n",
      "Epoch 75, Train Loss: 274.99, Test Loss: 1213.06,                   Time elapsed [s]: 27.64\n",
      "Epoch 76, Train Loss: 279.44, Test Loss: 1173.94,                   Time elapsed [s]: 27.59\n",
      "Epoch 77, Train Loss: 271.73, Test Loss: 1338.87,                   Time elapsed [s]: 28.97\n",
      "Epoch 78, Train Loss: 271.99, Test Loss: 1140.66,                   Time elapsed [s]: 27.29\n",
      "Epoch 79, Train Loss: 274.35, Test Loss: 1192.20,                   Time elapsed [s]: 27.10\n",
      "Epoch 80, Train Loss: 271.32, Test Loss: 1173.53,                   Time elapsed [s]: 27.08\n",
      "Epoch 81, Train Loss: 275.45, Test Loss: 1239.47,                   Time elapsed [s]: 27.36\n",
      "Epoch 82, Train Loss: 269.55, Test Loss: 1228.50,                   Time elapsed [s]: 27.25\n",
      "Epoch 83, Train Loss: 266.26, Test Loss: 1214.25,                   Time elapsed [s]: 27.39\n",
      "Epoch 84, Train Loss: 261.16, Test Loss: 1400.35,                   Time elapsed [s]: 27.47\n",
      "Epoch 85, Train Loss: 282.05, Test Loss: 1078.59,                   Time elapsed [s]: 27.59\n",
      "Epoch 86, Train Loss: 270.61, Test Loss: 1207.84,                   Time elapsed [s]: 28.09\n",
      "Epoch 87, Train Loss: 259.90, Test Loss: 1365.73,                   Time elapsed [s]: 29.85\n",
      "Epoch 88, Train Loss: 272.27, Test Loss: 1064.87,                   Time elapsed [s]: 28.71\n",
      "Epoch 89, Train Loss: 256.49, Test Loss: 1113.12,                   Time elapsed [s]: 27.52\n",
      "Epoch 90, Train Loss: 252.39, Test Loss: 1116.61,                   Time elapsed [s]: 27.47\n",
      "Epoch 91, Train Loss: 251.38, Test Loss: 1109.13,                   Time elapsed [s]: 27.94\n",
      "Epoch 92, Train Loss: 258.65, Test Loss: 1102.28,                   Time elapsed [s]: 28.84\n",
      "Epoch 93, Train Loss: 251.49, Test Loss: 1104.35,                   Time elapsed [s]: 28.61\n",
      "Epoch 94, Train Loss: 251.29, Test Loss: 1044.38,                   Time elapsed [s]: 28.55\n",
      "Epoch 95, Train Loss: 246.56, Test Loss: 1049.34,                   Time elapsed [s]: 27.65\n",
      "Epoch 96, Train Loss: 243.67, Test Loss: 983.74,                   Time elapsed [s]: 27.61\n",
      "Epoch 97, Train Loss: 253.02, Test Loss: 1068.74,                   Time elapsed [s]: 27.40\n",
      "Epoch 98, Train Loss: 241.95, Test Loss: 1085.49,                   Time elapsed [s]: 29.17\n",
      "Epoch 99, Train Loss: 273.78, Test Loss: 1125.01,                   Time elapsed [s]: 27.49\n",
      "Epoch 100, Train Loss: 259.08, Test Loss: 955.49,                   Time elapsed [s]: 27.73\n",
      "Epoch 101, Train Loss: 242.52, Test Loss: 948.12,                   Time elapsed [s]: 27.80\n",
      "Epoch 102, Train Loss: 230.05, Test Loss: 935.10,                   Time elapsed [s]: 27.97\n",
      "Epoch 103, Train Loss: 229.66, Test Loss: 929.11,                   Time elapsed [s]: 28.38\n",
      "Epoch 104, Train Loss: 225.71, Test Loss: 925.30,                   Time elapsed [s]: 28.45\n",
      "Epoch 105, Train Loss: 222.86, Test Loss: 928.18,                   Time elapsed [s]: 30.75\n",
      "Epoch 106, Train Loss: 224.06, Test Loss: 912.59,                   Time elapsed [s]: 28.58\n",
      "Epoch 107, Train Loss: 219.41, Test Loss: 941.28,                   Time elapsed [s]: 36.10\n",
      "Epoch 108, Train Loss: 227.87, Test Loss: 858.58,                   Time elapsed [s]: 28.78\n",
      "Epoch 109, Train Loss: 223.20, Test Loss: 909.64,                   Time elapsed [s]: 29.24\n",
      "Epoch 110, Train Loss: 229.25, Test Loss: 953.04,                   Time elapsed [s]: 30.08\n",
      "Epoch 111, Train Loss: 231.55, Test Loss: 913.04,                   Time elapsed [s]: 28.53\n",
      "Epoch 112, Train Loss: 233.35, Test Loss: 880.08,                   Time elapsed [s]: 29.01\n",
      "Epoch 113, Train Loss: 231.40, Test Loss: 969.68,                   Time elapsed [s]: 28.07\n",
      "Epoch 114, Train Loss: 226.21, Test Loss: 866.84,                   Time elapsed [s]: 27.87\n",
      "Epoch 115, Train Loss: 223.26, Test Loss: 848.80,                   Time elapsed [s]: 29.83\n",
      "Epoch 116, Train Loss: 213.50, Test Loss: 928.56,                   Time elapsed [s]: 31.10\n",
      "Epoch 117, Train Loss: 217.11, Test Loss: 870.79,                   Time elapsed [s]: 28.44\n",
      "Epoch 118, Train Loss: 219.82, Test Loss: 854.43,                   Time elapsed [s]: 27.65\n",
      "Epoch 119, Train Loss: 215.77, Test Loss: 862.20,                   Time elapsed [s]: 29.11\n",
      "Epoch 120, Train Loss: 214.21, Test Loss: 862.68,                   Time elapsed [s]: 27.88\n",
      "Epoch 121, Train Loss: 213.20, Test Loss: 879.28,                   Time elapsed [s]: 29.84\n",
      "Epoch 122, Train Loss: 213.96, Test Loss: 865.03,                   Time elapsed [s]: 28.01\n",
      "Epoch 123, Train Loss: 221.42, Test Loss: 847.45,                   Time elapsed [s]: 27.79\n",
      "Epoch 124, Train Loss: 215.74, Test Loss: 854.59,                   Time elapsed [s]: 28.70\n",
      "Epoch 125, Train Loss: 228.26, Test Loss: 858.05,                   Time elapsed [s]: 32.73\n",
      "Epoch 126, Train Loss: 216.84, Test Loss: 860.32,                   Time elapsed [s]: 29.30\n",
      "Epoch 127, Train Loss: 221.20, Test Loss: 928.29,                   Time elapsed [s]: 30.75\n",
      "Epoch 128, Train Loss: 218.64, Test Loss: 868.46,                   Time elapsed [s]: 28.84\n",
      "Epoch 129, Train Loss: 211.49, Test Loss: 907.12,                   Time elapsed [s]: 28.40\n",
      "Epoch 130, Train Loss: 222.42, Test Loss: 903.81,                   Time elapsed [s]: 28.99\n",
      "Epoch 131, Train Loss: 217.20, Test Loss: 882.75,                   Time elapsed [s]: 28.09\n",
      "Epoch 132, Train Loss: 216.68, Test Loss: 875.72,                   Time elapsed [s]: 27.81\n",
      "Epoch 133, Train Loss: 221.02, Test Loss: 802.19,                   Time elapsed [s]: 28.80\n",
      "Epoch 134, Train Loss: 219.03, Test Loss: 836.63,                   Time elapsed [s]: 29.06\n",
      "Epoch 135, Train Loss: 210.51, Test Loss: 847.27,                   Time elapsed [s]: 28.63\n",
      "Epoch 136, Train Loss: 215.48, Test Loss: 841.20,                   Time elapsed [s]: 27.72\n",
      "Epoch 137, Train Loss: 212.53, Test Loss: 829.67,                   Time elapsed [s]: 28.23\n",
      "Epoch 138, Train Loss: 214.24, Test Loss: 862.72,                   Time elapsed [s]: 28.44\n",
      "Epoch 139, Train Loss: 213.21, Test Loss: 825.57,                   Time elapsed [s]: 28.40\n",
      "Epoch 140, Train Loss: 211.36, Test Loss: 831.48,                   Time elapsed [s]: 28.85\n",
      "Epoch 141, Train Loss: 214.28, Test Loss: 834.92,                   Time elapsed [s]: 29.42\n",
      "Epoch 142, Train Loss: 211.12, Test Loss: 792.13,                   Time elapsed [s]: 29.03\n",
      "Epoch 143, Train Loss: 213.04, Test Loss: 817.09,                   Time elapsed [s]: 29.27\n",
      "Epoch 144, Train Loss: 210.21, Test Loss: 851.76,                   Time elapsed [s]: 29.48\n",
      "Epoch 145, Train Loss: 213.67, Test Loss: 808.66,                   Time elapsed [s]: 29.21\n",
      "Epoch 146, Train Loss: 209.99, Test Loss: 831.76,                   Time elapsed [s]: 28.44\n",
      "Epoch 147, Train Loss: 212.90, Test Loss: 813.21,                   Time elapsed [s]: 29.10\n",
      "Epoch 148, Train Loss: 209.27, Test Loss: 782.84,                   Time elapsed [s]: 28.73\n",
      "Epoch 149, Train Loss: 211.60, Test Loss: 798.89,                   Time elapsed [s]: 28.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_EPOCHS= 150\n",
    "save_path_model= './models/Farrow/scratch/%s/vae_frac%.1f_scaled_best_run%i.pt'\n",
    "save_path_losses = './models/Farrow/scratch/%s/losses_frac%.1f_scaled_run%i.npy'\n",
    "save_path_elapsed_time = './models/Farrow/scratch/%s/elapsed_time_frac%.1f_scaled_run%i.npy'\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\n",
    "for frac in [0.]:  \n",
    "    runs = range(1,4)\n",
    "        \n",
    "    for run in runs:\n",
    "        \n",
    "        \n",
    "        if os.path.exists(save_path_model%(part,frac,run)):\n",
    "            state_dict = torch.load(save_path_model%(part,frac,run))\n",
    "        else:\n",
    "            state_dict = None\n",
    "        #optimizer\n",
    "        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "        # load pre-trained model\n",
    "        if state_dict is not None:\n",
    "            model.load_state_dict(state_dict['model_state_dict'])\n",
    "            \n",
    "            optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "            classifier.load_state_dict(state_dict['classifier_state_dict'])\n",
    "            losses = np.load(save_path_losses%(part, frac, run))\n",
    "            elapsed_time = np.load(save_path_elapsed_time%(part, frac, run))\n",
    "            \n",
    "            last_epoch = state_dict['epoch']\n",
    "            training = list(losses[:last_epoch,:2])\n",
    "            validation = list(losses[:last_epoch,2:])\n",
    "            \n",
    "            best_test_loss = losses[:,2].min()\n",
    "            \n",
    "        else:\n",
    "            model.apply(init_weights)\n",
    "            classifier.apply(init_weights)\n",
    "            best_test_loss = np.infty\n",
    "            \n",
    "            training = []\n",
    "            validation=[]\n",
    "            last_epoch = 0\n",
    "            elapsed_time = np.zeros((N_EPOCHS,))\n",
    "            \n",
    "        \n",
    "        cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=-100)\n",
    "        mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "        \n",
    "        for e in range(last_epoch, N_EPOCHS):\n",
    "            start.record()\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, clip=1, norm_p=None,\n",
    "                                                 class_fraction=frac)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss, norm_p=None)\n",
    "            \n",
    "            end.record()\n",
    "\n",
    "            # Waits for everything to finish running\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time[e] = start.elapsed_time(end) # milliseconds\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "\n",
    "            training += [[train_loss,train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, \\\n",
    "                  Time elapsed [s]: {elapsed_time[e]/1000:.2f}')\n",
    "            \n",
    "            \n",
    "\n",
    "            if e % 50 == 0 and e > 0:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "\n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                torch.save({'epoch': e,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'classifier_state_dict': classifier.state_dict()\n",
    "                               },save_path_model%(part,frac, run))\n",
    "\n",
    "                validation_ = np.array(validation)\n",
    "                training_ = np.array(training)\n",
    "                # [:,0] = training loss, [:,1] = training classification losss\n",
    "                # [:,2] validation loss, [:,3] validation classification loss\n",
    "                losses = np.hstack((training_, validation_))\n",
    "                np.save(save_path_losses%(part,frac, run),losses)\n",
    "                np.save(save_path_elapsed_time%(part,frac, run),elapsed_time)\n",
    "                \n",
    "            \n",
    "            \n",
    "        validation = np.array(validation)\n",
    "        training = np.array(training)\n",
    "        losses = np.hstack((training, validation))\n",
    "        np.save(save_path_losses%(part,frac, run), losses)\n",
    "        np.save(save_path_elapsed_time%(part,frac, run),elapsed_time)\n",
    "        \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa1c8eb1410>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(elapsed_time/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.025633125\n",
      "1.1423353606102364\n"
     ]
    }
   ],
   "source": [
    "print(elapsed_time.mean()/1000)\n",
    "print(elapsed_time.std()/1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
