{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from utils.vmf_batch import vMF\n",
    "\n",
    "from models import SeqEncoder, SeqDecoder, Seq2Seq_VAE, PoolingClassifier, init_weights\n",
    "from utils.training_utils import train, evaluate\n",
    "\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Farrow_data/iterator/soma_centered/train_iterator.pkl', 'rb') as f:\n",
    "    train_iterator = pickle.load(f)\n",
    "\n",
    "with open('./data/Farrow_data/iterator/soma_centered/val_iterator.pkl', 'rb') as f:\n",
    "    val_iterator = pickle.load(f)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data, trg_data, seq_len, indices, labels = list(train_iterator)[0]\n",
    "bs, n_walks, walk_length, output_dim = src_data.shape\n",
    "\n",
    "N_train = len(train_iterator.sampler.indices)\n",
    "N_val = len(val_iterator.sampler.indices)\n",
    "\n",
    " \n",
    "MASKING_ELEMENT = train_iterator.dataset.masking_el\n",
    "\n",
    "# get number of labels, ignore -100 index\n",
    "l = list(np.unique(labels))\n",
    "if -100 in l:\n",
    "    l.remove(-100)\n",
    "NUM_CLASSES = len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100,    0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
       "         10,   11,   12,   13])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_iterator.dataset.labels[train_iterator.sampler.indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 45.709938049316406\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 32\n",
    "latent_dim = 32\n",
    "NUM_LAYERS = 2\n",
    "dpout = .1\n",
    "kap = 500\n",
    "pool = 'max'\n",
    "lr = 0.01\n",
    "\n",
    "enc = SeqEncoder(output_dim, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "dec = SeqDecoder(output_dim, emb_dim, emb_dim, NUM_LAYERS, dpout)\n",
    "dist = vMF(latent_dim, kappa=kap)\n",
    "model = Seq2Seq_VAE(enc, dec, dist, device).to(device)\n",
    "classifier = PoolingClassifier(latent_dim, NUM_CLASSES, n_walks,dpout,pooling=pool).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, ignore_el=MASKING_ELEMENT):\n",
    "    # reconstruction loss\n",
    "    # x = [trg len, batch size * n walks, output dim]\n",
    "\n",
    "    seq_len , bs, output_dim = x.shape\n",
    "    mask = x[:,:,0] != ignore_el\n",
    "    RCL = 0\n",
    "    for d in range(output_dim):\n",
    "        RCL += mse_loss(reconstructed_x[:,:,d][mask], x[:,:,d][mask])\n",
    "    RCL /= output_dim\n",
    "    \n",
    "    return RCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./models/Farrow/finetuned/soma_centered\"\n",
    "import os \n",
    "os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 65.46, Val Loss: 393.71, Time elapsed [s]: 19.29\n",
      "Epoch 1, Train Loss: 62.30, Val Loss: 412.80, Time elapsed [s]: 19.15\n",
      "Epoch 2, Train Loss: 67.77, Val Loss: 391.99, Time elapsed [s]: 19.16\n",
      "Epoch 3, Train Loss: 63.52, Val Loss: 380.41, Time elapsed [s]: 19.13\n",
      "Epoch 4, Train Loss: 64.58, Val Loss: 376.85, Time elapsed [s]: 19.10\n",
      "Epoch 5, Train Loss: 64.51, Val Loss: 386.56, Time elapsed [s]: 19.13\n",
      "Epoch 6, Train Loss: 59.44, Val Loss: 382.96, Time elapsed [s]: 19.18\n",
      "Epoch 7, Train Loss: 56.94, Val Loss: 401.25, Time elapsed [s]: 19.07\n",
      "Epoch 8, Train Loss: 61.06, Val Loss: 393.43, Time elapsed [s]: 19.15\n",
      "Epoch 9, Train Loss: 59.21, Val Loss: 399.39, Time elapsed [s]: 19.11\n",
      "Epoch 10, Train Loss: 65.00, Val Loss: 422.77, Time elapsed [s]: 19.03\n",
      "Epoch 11, Train Loss: 61.88, Val Loss: 424.27, Time elapsed [s]: 19.15\n",
      "Epoch 12, Train Loss: 64.01, Val Loss: 433.07, Time elapsed [s]: 19.16\n",
      "Epoch 13, Train Loss: 63.72, Val Loss: 451.85, Time elapsed [s]: 19.20\n",
      "Epoch 14, Train Loss: 65.48, Val Loss: 533.60, Time elapsed [s]: 19.03\n",
      "Epoch 15, Train Loss: 63.14, Val Loss: 548.97, Time elapsed [s]: 19.09\n",
      "Epoch 16, Train Loss: 65.85, Val Loss: 502.84, Time elapsed [s]: 19.08\n",
      "Epoch 17, Train Loss: 61.53, Val Loss: 464.46, Time elapsed [s]: 18.97\n",
      "Epoch 18, Train Loss: 62.67, Val Loss: 467.97, Time elapsed [s]: 19.08\n",
      "Epoch 19, Train Loss: 64.95, Val Loss: 485.77, Time elapsed [s]: 19.08\n",
      "Epoch 20, Train Loss: 65.93, Val Loss: 424.76, Time elapsed [s]: 19.05\n",
      "Epoch 21, Train Loss: 59.68, Val Loss: 422.27, Time elapsed [s]: 19.23\n",
      "Epoch 22, Train Loss: 62.04, Val Loss: 393.87, Time elapsed [s]: 19.04\n",
      "Epoch 23, Train Loss: 59.97, Val Loss: 407.62, Time elapsed [s]: 19.09\n",
      "Epoch 24, Train Loss: 61.46, Val Loss: 474.33, Time elapsed [s]: 19.06\n",
      "Epoch 25, Train Loss: 68.44, Val Loss: 511.66, Time elapsed [s]: 18.98\n",
      "Epoch 26, Train Loss: 66.26, Val Loss: 459.10, Time elapsed [s]: 19.07\n",
      "Epoch 27, Train Loss: 63.92, Val Loss: 458.87, Time elapsed [s]: 19.13\n",
      "Epoch 28, Train Loss: 67.31, Val Loss: 426.09, Time elapsed [s]: 19.13\n",
      "Epoch 29, Train Loss: 63.97, Val Loss: 448.48, Time elapsed [s]: 19.04\n",
      "Epoch 30, Train Loss: 63.52, Val Loss: 426.09, Time elapsed [s]: 19.00\n",
      "Epoch 31, Train Loss: 59.52, Val Loss: 455.73, Time elapsed [s]: 19.14\n",
      "Epoch 32, Train Loss: 60.78, Val Loss: 450.89, Time elapsed [s]: 19.07\n",
      "Epoch 33, Train Loss: 58.86, Val Loss: 483.03, Time elapsed [s]: 19.09\n",
      "Epoch 34, Train Loss: 64.62, Val Loss: 451.78, Time elapsed [s]: 19.16\n",
      "Epoch 35, Train Loss: 52.62, Val Loss: 464.86, Time elapsed [s]: 19.09\n",
      "Epoch 36, Train Loss: 56.99, Val Loss: 459.38, Time elapsed [s]: 19.05\n",
      "Epoch 37, Train Loss: 70.38, Val Loss: 435.85, Time elapsed [s]: 19.12\n",
      "Epoch 38, Train Loss: 67.87, Val Loss: 413.77, Time elapsed [s]: 19.06\n",
      "Epoch 39, Train Loss: 66.07, Val Loss: 440.13, Time elapsed [s]: 19.09\n",
      "Epoch 40, Train Loss: 69.05, Val Loss: 420.69, Time elapsed [s]: 19.04\n",
      "Epoch 41, Train Loss: 61.40, Val Loss: 421.69, Time elapsed [s]: 19.03\n",
      "Epoch 42, Train Loss: 65.62, Val Loss: 476.08, Time elapsed [s]: 19.06\n",
      "Epoch 43, Train Loss: 67.45, Val Loss: 452.61, Time elapsed [s]: 19.14\n",
      "Epoch 44, Train Loss: 58.45, Val Loss: 392.91, Time elapsed [s]: 19.07\n",
      "Epoch 45, Train Loss: 53.13, Val Loss: 389.50, Time elapsed [s]: 19.05\n",
      "Epoch 46, Train Loss: 54.47, Val Loss: 389.36, Time elapsed [s]: 19.06\n",
      "Epoch 47, Train Loss: 62.33, Val Loss: 395.33, Time elapsed [s]: 19.16\n",
      "Epoch 48, Train Loss: 60.75, Val Loss: 405.99, Time elapsed [s]: 19.12\n",
      "Epoch 49, Train Loss: 62.82, Val Loss: 412.52, Time elapsed [s]: 19.13\n",
      "Epoch 0, Train Loss: 127.09, Val Loss: 566.07, Time elapsed [s]: 19.11\n",
      "Epoch 1, Train Loss: 120.22, Val Loss: 600.91, Time elapsed [s]: 19.11\n",
      "Epoch 2, Train Loss: 119.08, Val Loss: 575.09, Time elapsed [s]: 19.10\n",
      "Epoch 3, Train Loss: 120.39, Val Loss: 529.27, Time elapsed [s]: 19.14\n",
      "Epoch 4, Train Loss: 108.36, Val Loss: 549.70, Time elapsed [s]: 19.01\n",
      "Epoch 5, Train Loss: 110.27, Val Loss: 595.35, Time elapsed [s]: 19.07\n",
      "Epoch 6, Train Loss: 111.72, Val Loss: 608.13, Time elapsed [s]: 19.08\n",
      "Epoch 7, Train Loss: 109.18, Val Loss: 574.64, Time elapsed [s]: 19.04\n",
      "Epoch 8, Train Loss: 113.97, Val Loss: 541.29, Time elapsed [s]: 19.16\n",
      "Epoch 9, Train Loss: 102.90, Val Loss: 608.42, Time elapsed [s]: 19.11\n",
      "Epoch 10, Train Loss: 107.08, Val Loss: 642.52, Time elapsed [s]: 19.08\n",
      "Epoch 11, Train Loss: 109.48, Val Loss: 662.08, Time elapsed [s]: 19.10\n",
      "Epoch 12, Train Loss: 105.70, Val Loss: 629.53, Time elapsed [s]: 19.06\n",
      "Epoch 13, Train Loss: 111.17, Val Loss: 610.78, Time elapsed [s]: 19.02\n",
      "Epoch 14, Train Loss: 107.19, Val Loss: 581.02, Time elapsed [s]: 19.01\n",
      "Epoch 15, Train Loss: 104.59, Val Loss: 605.69, Time elapsed [s]: 19.07\n",
      "Epoch 16, Train Loss: 106.62, Val Loss: 559.19, Time elapsed [s]: 19.02\n",
      "Epoch 17, Train Loss: 96.83, Val Loss: 605.26, Time elapsed [s]: 19.02\n",
      "Epoch 18, Train Loss: 95.65, Val Loss: 709.31, Time elapsed [s]: 19.09\n",
      "Epoch 19, Train Loss: 103.69, Val Loss: 565.29, Time elapsed [s]: 19.03\n",
      "Epoch 20, Train Loss: 101.89, Val Loss: 503.08, Time elapsed [s]: 19.01\n",
      "Epoch 21, Train Loss: 111.45, Val Loss: 570.60, Time elapsed [s]: 19.02\n",
      "Epoch 22, Train Loss: 106.83, Val Loss: 583.11, Time elapsed [s]: 19.06\n",
      "Epoch 23, Train Loss: 107.21, Val Loss: 543.99, Time elapsed [s]: 19.04\n",
      "Epoch 24, Train Loss: 106.02, Val Loss: 511.14, Time elapsed [s]: 19.01\n",
      "Epoch 25, Train Loss: 99.52, Val Loss: 543.00, Time elapsed [s]: 18.94\n",
      "Epoch 26, Train Loss: 109.69, Val Loss: 558.68, Time elapsed [s]: 18.99\n",
      "Epoch 27, Train Loss: 97.60, Val Loss: 512.05, Time elapsed [s]: 19.01\n",
      "Epoch 28, Train Loss: 98.55, Val Loss: 525.24, Time elapsed [s]: 19.06\n",
      "Epoch 29, Train Loss: 94.02, Val Loss: 522.98, Time elapsed [s]: 19.04\n",
      "Epoch 30, Train Loss: 91.78, Val Loss: 541.64, Time elapsed [s]: 18.98\n",
      "Epoch 31, Train Loss: 93.83, Val Loss: 580.83, Time elapsed [s]: 18.97\n",
      "Epoch 32, Train Loss: 95.16, Val Loss: 632.46, Time elapsed [s]: 19.05\n",
      "Epoch 33, Train Loss: 106.50, Val Loss: 663.01, Time elapsed [s]: 18.98\n",
      "Epoch 34, Train Loss: 110.72, Val Loss: 621.52, Time elapsed [s]: 19.10\n",
      "Epoch 35, Train Loss: 107.98, Val Loss: 535.91, Time elapsed [s]: 19.02\n",
      "Epoch 36, Train Loss: 93.81, Val Loss: 487.90, Time elapsed [s]: 19.05\n",
      "Epoch 37, Train Loss: 83.27, Val Loss: 482.85, Time elapsed [s]: 19.03\n",
      "Epoch 38, Train Loss: 92.74, Val Loss: 560.70, Time elapsed [s]: 19.13\n",
      "Epoch 39, Train Loss: 89.54, Val Loss: 576.86, Time elapsed [s]: 19.06\n",
      "Epoch 40, Train Loss: 90.93, Val Loss: 625.92, Time elapsed [s]: 19.11\n",
      "Epoch 41, Train Loss: 90.35, Val Loss: 579.27, Time elapsed [s]: 19.08\n",
      "Epoch 42, Train Loss: 90.11, Val Loss: 647.91, Time elapsed [s]: 18.96\n",
      "Epoch 43, Train Loss: 93.94, Val Loss: 635.67, Time elapsed [s]: 18.98\n",
      "Epoch 44, Train Loss: 95.04, Val Loss: 521.80, Time elapsed [s]: 19.05\n",
      "Epoch 45, Train Loss: 99.05, Val Loss: 610.09, Time elapsed [s]: 19.03\n",
      "Epoch 46, Train Loss: 115.63, Val Loss: 580.74, Time elapsed [s]: 18.97\n",
      "Epoch 47, Train Loss: 104.56, Val Loss: 543.76, Time elapsed [s]: 18.99\n",
      "Epoch 48, Train Loss: 95.77, Val Loss: 519.85, Time elapsed [s]: 19.00\n",
      "Epoch 49, Train Loss: 86.12, Val Loss: 526.62, Time elapsed [s]: 18.98\n",
      "Epoch 0, Train Loss: 106.32, Val Loss: 666.49, Time elapsed [s]: 19.07\n",
      "Epoch 1, Train Loss: 113.36, Val Loss: 662.18, Time elapsed [s]: 19.06\n",
      "Epoch 2, Train Loss: 103.62, Val Loss: 647.55, Time elapsed [s]: 19.05\n",
      "Epoch 3, Train Loss: 104.56, Val Loss: 614.87, Time elapsed [s]: 19.02\n",
      "Epoch 4, Train Loss: 106.84, Val Loss: 643.90, Time elapsed [s]: 19.03\n",
      "Epoch 5, Train Loss: 100.06, Val Loss: 643.23, Time elapsed [s]: 18.99\n",
      "Epoch 6, Train Loss: 107.56, Val Loss: 560.05, Time elapsed [s]: 19.05\n",
      "Epoch 7, Train Loss: 106.75, Val Loss: 530.37, Time elapsed [s]: 19.06\n",
      "Epoch 8, Train Loss: 100.37, Val Loss: 557.43, Time elapsed [s]: 19.02\n",
      "Epoch 9, Train Loss: 96.25, Val Loss: 534.37, Time elapsed [s]: 19.06\n",
      "Epoch 10, Train Loss: 89.16, Val Loss: 503.58, Time elapsed [s]: 19.06\n",
      "Epoch 11, Train Loss: 88.78, Val Loss: 632.16, Time elapsed [s]: 19.10\n",
      "Epoch 12, Train Loss: 90.30, Val Loss: 801.42, Time elapsed [s]: 19.07\n",
      "Epoch 13, Train Loss: 94.33, Val Loss: 754.31, Time elapsed [s]: 18.98\n",
      "Epoch 14, Train Loss: 100.95, Val Loss: 714.24, Time elapsed [s]: 18.97\n",
      "Epoch 15, Train Loss: 102.19, Val Loss: 667.89, Time elapsed [s]: 19.15\n",
      "Epoch 16, Train Loss: 95.49, Val Loss: 582.71, Time elapsed [s]: 18.98\n",
      "Epoch 17, Train Loss: 106.56, Val Loss: 524.20, Time elapsed [s]: 19.09\n",
      "Epoch 18, Train Loss: 97.40, Val Loss: 552.00, Time elapsed [s]: 19.02\n",
      "Epoch 19, Train Loss: 94.78, Val Loss: 510.64, Time elapsed [s]: 19.07\n",
      "Epoch 20, Train Loss: 93.10, Val Loss: 616.09, Time elapsed [s]: 19.05\n",
      "Epoch 21, Train Loss: 94.63, Val Loss: 562.35, Time elapsed [s]: 19.07\n",
      "Epoch 22, Train Loss: 91.42, Val Loss: 559.98, Time elapsed [s]: 18.99\n",
      "Epoch 23, Train Loss: 88.64, Val Loss: 584.99, Time elapsed [s]: 19.17\n",
      "Epoch 24, Train Loss: 90.30, Val Loss: 664.65, Time elapsed [s]: 19.06\n",
      "Epoch 25, Train Loss: 89.53, Val Loss: 642.87, Time elapsed [s]: 19.16\n",
      "Epoch 26, Train Loss: 87.40, Val Loss: 545.04, Time elapsed [s]: 19.15\n",
      "Epoch 27, Train Loss: 90.66, Val Loss: 455.79, Time elapsed [s]: 19.08\n",
      "Epoch 28, Train Loss: 95.85, Val Loss: 480.61, Time elapsed [s]: 19.14\n",
      "Epoch 29, Train Loss: 84.75, Val Loss: 490.84, Time elapsed [s]: 19.01\n",
      "Epoch 30, Train Loss: 88.00, Val Loss: 476.58, Time elapsed [s]: 19.03\n",
      "Epoch 31, Train Loss: 85.71, Val Loss: 538.55, Time elapsed [s]: 19.05\n",
      "Epoch 32, Train Loss: 84.58, Val Loss: 531.56, Time elapsed [s]: 19.00\n",
      "Epoch 33, Train Loss: 84.79, Val Loss: 493.30, Time elapsed [s]: 19.01\n",
      "Epoch 34, Train Loss: 85.84, Val Loss: 429.60, Time elapsed [s]: 19.14\n",
      "Epoch 35, Train Loss: 75.00, Val Loss: 531.20, Time elapsed [s]: 19.08\n",
      "Epoch 36, Train Loss: 78.11, Val Loss: 543.94, Time elapsed [s]: 19.13\n",
      "Epoch 37, Train Loss: 78.48, Val Loss: 711.50, Time elapsed [s]: 19.03\n",
      "Epoch 38, Train Loss: 88.71, Val Loss: 517.65, Time elapsed [s]: 19.01\n",
      "Epoch 39, Train Loss: 84.99, Val Loss: 530.76, Time elapsed [s]: 19.12\n",
      "Epoch 40, Train Loss: 81.37, Val Loss: 434.14, Time elapsed [s]: 18.97\n",
      "Epoch 41, Train Loss: 83.41, Val Loss: 467.00, Time elapsed [s]: 19.07\n",
      "Epoch 42, Train Loss: 84.10, Val Loss: 633.79, Time elapsed [s]: 19.12\n",
      "Epoch 43, Train Loss: 81.92, Val Loss: 599.27, Time elapsed [s]: 19.14\n",
      "Epoch 44, Train Loss: 88.51, Val Loss: 607.32, Time elapsed [s]: 19.07\n",
      "Epoch 45, Train Loss: 84.57, Val Loss: 591.30, Time elapsed [s]: 19.15\n",
      "Epoch 46, Train Loss: 77.70, Val Loss: 479.06, Time elapsed [s]: 19.04\n",
      "Epoch 47, Train Loss: 76.31, Val Loss: 519.66, Time elapsed [s]: 19.04\n",
      "Epoch 48, Train Loss: 88.68, Val Loss: 512.51, Time elapsed [s]: 19.04\n",
      "Epoch 49, Train Loss: 96.16, Val Loss: 469.64, Time elapsed [s]: 19.02\n",
      "Epoch 0, Train Loss: 111.05, Val Loss: 628.13, Time elapsed [s]: 19.29\n",
      "Epoch 1, Train Loss: 114.98, Val Loss: 633.96, Time elapsed [s]: 19.15\n",
      "Epoch 2, Train Loss: 115.78, Val Loss: 599.57, Time elapsed [s]: 19.16\n",
      "Epoch 3, Train Loss: 121.29, Val Loss: 562.10, Time elapsed [s]: 19.13\n",
      "Epoch 4, Train Loss: 119.10, Val Loss: 580.01, Time elapsed [s]: 19.10\n",
      "Epoch 5, Train Loss: 112.08, Val Loss: 601.46, Time elapsed [s]: 19.13\n",
      "Epoch 6, Train Loss: 122.64, Val Loss: 556.13, Time elapsed [s]: 19.18\n",
      "Epoch 7, Train Loss: 116.96, Val Loss: 537.28, Time elapsed [s]: 19.07\n",
      "Epoch 8, Train Loss: 116.23, Val Loss: 577.04, Time elapsed [s]: 19.15\n",
      "Epoch 9, Train Loss: 109.24, Val Loss: 573.12, Time elapsed [s]: 19.11\n",
      "Epoch 10, Train Loss: 108.89, Val Loss: 537.45, Time elapsed [s]: 19.03\n",
      "Epoch 11, Train Loss: 104.07, Val Loss: 582.27, Time elapsed [s]: 19.15\n",
      "Epoch 12, Train Loss: 111.81, Val Loss: 539.57, Time elapsed [s]: 19.16\n",
      "Epoch 13, Train Loss: 109.73, Val Loss: 519.96, Time elapsed [s]: 19.20\n",
      "Epoch 14, Train Loss: 106.11, Val Loss: 511.47, Time elapsed [s]: 19.03\n",
      "Epoch 15, Train Loss: 106.51, Val Loss: 530.14, Time elapsed [s]: 19.09\n",
      "Epoch 16, Train Loss: 108.00, Val Loss: 582.96, Time elapsed [s]: 19.08\n",
      "Epoch 17, Train Loss: 106.92, Val Loss: 539.06, Time elapsed [s]: 18.97\n",
      "Epoch 18, Train Loss: 107.23, Val Loss: 522.24, Time elapsed [s]: 19.08\n",
      "Epoch 19, Train Loss: 107.20, Val Loss: 554.36, Time elapsed [s]: 19.08\n",
      "Epoch 20, Train Loss: 109.40, Val Loss: 499.75, Time elapsed [s]: 19.05\n",
      "Epoch 21, Train Loss: 104.47, Val Loss: 511.92, Time elapsed [s]: 19.23\n",
      "Epoch 22, Train Loss: 103.25, Val Loss: 482.21, Time elapsed [s]: 19.04\n",
      "Epoch 23, Train Loss: 97.91, Val Loss: 548.90, Time elapsed [s]: 19.09\n",
      "Epoch 24, Train Loss: 99.86, Val Loss: 541.45, Time elapsed [s]: 19.06\n",
      "Epoch 25, Train Loss: 106.18, Val Loss: 552.21, Time elapsed [s]: 18.98\n",
      "Epoch 26, Train Loss: 100.86, Val Loss: 546.06, Time elapsed [s]: 19.07\n",
      "Epoch 27, Train Loss: 98.01, Val Loss: 521.94, Time elapsed [s]: 19.13\n",
      "Epoch 28, Train Loss: 91.51, Val Loss: 551.83, Time elapsed [s]: 19.13\n",
      "Epoch 29, Train Loss: 92.03, Val Loss: 631.28, Time elapsed [s]: 19.04\n",
      "Epoch 30, Train Loss: 103.72, Val Loss: 562.97, Time elapsed [s]: 19.00\n",
      "Epoch 31, Train Loss: 111.45, Val Loss: 535.48, Time elapsed [s]: 19.14\n",
      "Epoch 32, Train Loss: 113.47, Val Loss: 523.54, Time elapsed [s]: 19.07\n",
      "Epoch 33, Train Loss: 100.92, Val Loss: 555.67, Time elapsed [s]: 19.09\n",
      "Epoch 34, Train Loss: 99.60, Val Loss: 546.09, Time elapsed [s]: 19.16\n",
      "Epoch 35, Train Loss: 97.57, Val Loss: 492.85, Time elapsed [s]: 19.09\n",
      "Epoch 36, Train Loss: 96.09, Val Loss: 479.77, Time elapsed [s]: 19.05\n",
      "Epoch 37, Train Loss: 90.58, Val Loss: 483.71, Time elapsed [s]: 19.12\n",
      "Epoch 38, Train Loss: 91.26, Val Loss: 553.41, Time elapsed [s]: 19.06\n",
      "Epoch 39, Train Loss: 92.72, Val Loss: 589.78, Time elapsed [s]: 19.09\n",
      "Epoch 40, Train Loss: 97.81, Val Loss: 520.74, Time elapsed [s]: 19.04\n",
      "Epoch 41, Train Loss: 105.41, Val Loss: 516.18, Time elapsed [s]: 19.03\n",
      "Epoch 42, Train Loss: 102.70, Val Loss: 509.26, Time elapsed [s]: 19.06\n",
      "Epoch 43, Train Loss: 91.90, Val Loss: 544.92, Time elapsed [s]: 19.14\n",
      "Epoch 44, Train Loss: 89.08, Val Loss: 536.09, Time elapsed [s]: 19.07\n",
      "Epoch 45, Train Loss: 87.47, Val Loss: 482.43, Time elapsed [s]: 19.05\n",
      "Epoch 46, Train Loss: 82.72, Val Loss: 492.43, Time elapsed [s]: 19.06\n",
      "Epoch 47, Train Loss: 86.69, Val Loss: 534.96, Time elapsed [s]: 19.16\n",
      "Epoch 48, Train Loss: 94.78, Val Loss: 498.28, Time elapsed [s]: 19.12\n",
      "Epoch 49, Train Loss: 89.62, Val Loss: 504.11, Time elapsed [s]: 19.13\n",
      "Epoch 0, Train Loss: 112.29, Val Loss: 604.62, Time elapsed [s]: 19.11\n",
      "Epoch 1, Train Loss: 106.05, Val Loss: 803.18, Time elapsed [s]: 19.11\n",
      "Epoch 2, Train Loss: 112.80, Val Loss: 629.02, Time elapsed [s]: 19.10\n",
      "Epoch 3, Train Loss: 108.08, Val Loss: 660.51, Time elapsed [s]: 19.14\n",
      "Epoch 4, Train Loss: 113.56, Val Loss: 658.17, Time elapsed [s]: 19.01\n",
      "Epoch 5, Train Loss: 102.55, Val Loss: 935.86, Time elapsed [s]: 19.07\n",
      "Epoch 6, Train Loss: 105.91, Val Loss: 956.85, Time elapsed [s]: 19.08\n",
      "Epoch 7, Train Loss: 110.74, Val Loss: 1124.28, Time elapsed [s]: 19.04\n",
      "Epoch 8, Train Loss: 111.97, Val Loss: 742.36, Time elapsed [s]: 19.16\n",
      "Epoch 9, Train Loss: 107.69, Val Loss: 776.44, Time elapsed [s]: 19.11\n",
      "Epoch 10, Train Loss: 120.75, Val Loss: 545.87, Time elapsed [s]: 19.08\n",
      "Epoch 11, Train Loss: 125.44, Val Loss: 700.55, Time elapsed [s]: 19.10\n",
      "Epoch 12, Train Loss: 115.43, Val Loss: 840.38, Time elapsed [s]: 19.06\n",
      "Epoch 13, Train Loss: 112.68, Val Loss: 691.55, Time elapsed [s]: 19.02\n",
      "Epoch 14, Train Loss: 122.48, Val Loss: 607.09, Time elapsed [s]: 19.01\n",
      "Epoch 15, Train Loss: 110.77, Val Loss: 605.74, Time elapsed [s]: 19.07\n",
      "Epoch 16, Train Loss: 103.15, Val Loss: 568.72, Time elapsed [s]: 19.02\n",
      "Epoch 17, Train Loss: 101.58, Val Loss: 494.98, Time elapsed [s]: 19.02\n",
      "Epoch 18, Train Loss: 97.58, Val Loss: 610.73, Time elapsed [s]: 19.09\n",
      "Epoch 19, Train Loss: 100.48, Val Loss: 764.53, Time elapsed [s]: 19.03\n",
      "Epoch 20, Train Loss: 109.58, Val Loss: 810.56, Time elapsed [s]: 19.01\n",
      "Epoch 21, Train Loss: 120.89, Val Loss: 598.40, Time elapsed [s]: 19.02\n",
      "Epoch 22, Train Loss: 119.99, Val Loss: 667.73, Time elapsed [s]: 19.06\n",
      "Epoch 23, Train Loss: 109.02, Val Loss: 658.12, Time elapsed [s]: 19.04\n",
      "Epoch 24, Train Loss: 107.26, Val Loss: 729.27, Time elapsed [s]: 19.01\n",
      "Epoch 25, Train Loss: 102.35, Val Loss: 745.46, Time elapsed [s]: 18.94\n",
      "Epoch 26, Train Loss: 102.62, Val Loss: 909.57, Time elapsed [s]: 18.99\n",
      "Epoch 27, Train Loss: 113.53, Val Loss: 668.38, Time elapsed [s]: 19.01\n",
      "Epoch 28, Train Loss: 119.99, Val Loss: 650.60, Time elapsed [s]: 19.06\n",
      "Epoch 29, Train Loss: 107.29, Val Loss: 564.40, Time elapsed [s]: 19.04\n",
      "Epoch 30, Train Loss: 98.97, Val Loss: 524.15, Time elapsed [s]: 18.98\n",
      "Epoch 31, Train Loss: 94.19, Val Loss: 534.88, Time elapsed [s]: 18.97\n",
      "Epoch 32, Train Loss: 92.37, Val Loss: 597.43, Time elapsed [s]: 19.05\n",
      "Epoch 33, Train Loss: 95.29, Val Loss: 540.47, Time elapsed [s]: 18.98\n",
      "Epoch 34, Train Loss: 91.86, Val Loss: 546.83, Time elapsed [s]: 19.10\n",
      "Epoch 35, Train Loss: 93.23, Val Loss: 588.45, Time elapsed [s]: 19.02\n",
      "Epoch 36, Train Loss: 96.37, Val Loss: 712.73, Time elapsed [s]: 19.05\n",
      "Epoch 37, Train Loss: 96.13, Val Loss: 878.11, Time elapsed [s]: 19.03\n",
      "Epoch 38, Train Loss: 98.52, Val Loss: 953.46, Time elapsed [s]: 19.13\n",
      "Epoch 39, Train Loss: 115.38, Val Loss: 611.64, Time elapsed [s]: 19.06\n",
      "Epoch 40, Train Loss: 111.06, Val Loss: 605.89, Time elapsed [s]: 19.11\n",
      "Epoch 41, Train Loss: 100.48, Val Loss: 588.38, Time elapsed [s]: 19.08\n",
      "Epoch 42, Train Loss: 97.48, Val Loss: 548.72, Time elapsed [s]: 18.96\n",
      "Epoch 43, Train Loss: 97.01, Val Loss: 621.88, Time elapsed [s]: 18.98\n",
      "Epoch 44, Train Loss: 104.34, Val Loss: 540.58, Time elapsed [s]: 19.05\n",
      "Epoch 45, Train Loss: 94.25, Val Loss: 478.74, Time elapsed [s]: 19.03\n",
      "Epoch 46, Train Loss: 95.97, Val Loss: 524.35, Time elapsed [s]: 18.97\n",
      "Epoch 47, Train Loss: 104.51, Val Loss: 553.83, Time elapsed [s]: 18.99\n",
      "Epoch 48, Train Loss: 93.29, Val Loss: 504.94, Time elapsed [s]: 19.00\n",
      "Epoch 49, Train Loss: 101.39, Val Loss: 490.51, Time elapsed [s]: 18.98\n",
      "Epoch 0, Train Loss: 362.63, Val Loss: 1862.22, Time elapsed [s]: 19.07\n",
      "Epoch 1, Train Loss: 325.88, Val Loss: 1489.35, Time elapsed [s]: 19.06\n",
      "Epoch 2, Train Loss: 272.33, Val Loss: 1767.53, Time elapsed [s]: 19.05\n",
      "Epoch 3, Train Loss: 263.29, Val Loss: 1469.55, Time elapsed [s]: 19.02\n",
      "Epoch 4, Train Loss: 249.74, Val Loss: 1368.33, Time elapsed [s]: 19.03\n",
      "Epoch 5, Train Loss: 225.60, Val Loss: 1404.50, Time elapsed [s]: 18.99\n",
      "Epoch 6, Train Loss: 206.27, Val Loss: 1318.09, Time elapsed [s]: 19.05\n",
      "Epoch 7, Train Loss: 199.04, Val Loss: 1474.71, Time elapsed [s]: 19.06\n",
      "Epoch 8, Train Loss: 194.77, Val Loss: 1191.75, Time elapsed [s]: 19.02\n",
      "Epoch 9, Train Loss: 202.82, Val Loss: 961.44, Time elapsed [s]: 19.06\n",
      "Epoch 10, Train Loss: 204.09, Val Loss: 1158.80, Time elapsed [s]: 19.06\n",
      "Epoch 11, Train Loss: 183.83, Val Loss: 953.72, Time elapsed [s]: 19.10\n",
      "Epoch 12, Train Loss: 193.08, Val Loss: 907.81, Time elapsed [s]: 19.07\n",
      "Epoch 13, Train Loss: 183.37, Val Loss: 1053.83, Time elapsed [s]: 18.98\n",
      "Epoch 14, Train Loss: 172.67, Val Loss: 1134.91, Time elapsed [s]: 18.97\n",
      "Epoch 15, Train Loss: 178.12, Val Loss: 1366.21, Time elapsed [s]: 19.15\n",
      "Epoch 16, Train Loss: 178.36, Val Loss: 873.38, Time elapsed [s]: 18.98\n",
      "Epoch 17, Train Loss: 169.22, Val Loss: 767.07, Time elapsed [s]: 19.09\n",
      "Epoch 18, Train Loss: 159.32, Val Loss: 856.34, Time elapsed [s]: 19.02\n",
      "Epoch 19, Train Loss: 158.22, Val Loss: 864.75, Time elapsed [s]: 19.07\n",
      "Epoch 20, Train Loss: 151.48, Val Loss: 925.39, Time elapsed [s]: 19.05\n",
      "Epoch 21, Train Loss: 151.04, Val Loss: 1107.86, Time elapsed [s]: 19.07\n",
      "Epoch 22, Train Loss: 147.40, Val Loss: 1087.12, Time elapsed [s]: 18.99\n",
      "Epoch 23, Train Loss: 153.94, Val Loss: 785.21, Time elapsed [s]: 19.17\n",
      "Epoch 24, Train Loss: 157.40, Val Loss: 827.05, Time elapsed [s]: 19.06\n",
      "Epoch 25, Train Loss: 143.02, Val Loss: 964.15, Time elapsed [s]: 19.16\n",
      "Epoch 26, Train Loss: 145.24, Val Loss: 672.65, Time elapsed [s]: 19.15\n",
      "Epoch 27, Train Loss: 127.88, Val Loss: 811.22, Time elapsed [s]: 19.08\n",
      "Epoch 28, Train Loss: 137.16, Val Loss: 699.35, Time elapsed [s]: 19.14\n",
      "Epoch 29, Train Loss: 134.24, Val Loss: 683.55, Time elapsed [s]: 19.01\n",
      "Epoch 30, Train Loss: 141.14, Val Loss: 738.78, Time elapsed [s]: 19.03\n",
      "Epoch 31, Train Loss: 150.88, Val Loss: 751.35, Time elapsed [s]: 19.05\n",
      "Epoch 32, Train Loss: 144.32, Val Loss: 768.48, Time elapsed [s]: 19.00\n",
      "Epoch 33, Train Loss: 132.86, Val Loss: 802.87, Time elapsed [s]: 19.01\n",
      "Epoch 34, Train Loss: 125.23, Val Loss: 801.19, Time elapsed [s]: 19.14\n",
      "Epoch 35, Train Loss: 128.43, Val Loss: 779.00, Time elapsed [s]: 19.08\n",
      "Epoch 36, Train Loss: 133.25, Val Loss: 652.12, Time elapsed [s]: 19.13\n",
      "Epoch 37, Train Loss: 115.20, Val Loss: 688.74, Time elapsed [s]: 19.03\n",
      "Epoch 38, Train Loss: 122.63, Val Loss: 873.37, Time elapsed [s]: 19.01\n",
      "Epoch 39, Train Loss: 118.99, Val Loss: 700.77, Time elapsed [s]: 19.12\n",
      "Epoch 40, Train Loss: 115.21, Val Loss: 765.51, Time elapsed [s]: 18.97\n",
      "Epoch 41, Train Loss: 111.81, Val Loss: 765.84, Time elapsed [s]: 19.07\n",
      "Epoch 42, Train Loss: 118.24, Val Loss: 663.65, Time elapsed [s]: 19.12\n",
      "Epoch 43, Train Loss: 126.12, Val Loss: 688.80, Time elapsed [s]: 19.14\n",
      "Epoch 44, Train Loss: 116.58, Val Loss: 694.64, Time elapsed [s]: 19.07\n",
      "Epoch 45, Train Loss: 111.77, Val Loss: 655.66, Time elapsed [s]: 19.15\n",
      "Epoch 46, Train Loss: 117.06, Val Loss: 566.08, Time elapsed [s]: 19.04\n",
      "Epoch 47, Train Loss: 128.60, Val Loss: 631.78, Time elapsed [s]: 19.04\n",
      "Epoch 48, Train Loss: 106.04, Val Loss: 498.14, Time elapsed [s]: 19.04\n",
      "Epoch 49, Train Loss: 98.54, Val Loss: 569.30, Time elapsed [s]: 19.02\n",
      "Epoch 0, Train Loss: 295.13, Val Loss: 2354.42, Time elapsed [s]: 19.29\n",
      "Epoch 1, Train Loss: 259.93, Val Loss: 1406.29, Time elapsed [s]: 19.15\n",
      "Epoch 2, Train Loss: 193.44, Val Loss: 1139.03, Time elapsed [s]: 19.16\n",
      "Epoch 3, Train Loss: 183.32, Val Loss: 1112.91, Time elapsed [s]: 19.13\n",
      "Epoch 4, Train Loss: 164.47, Val Loss: 1114.80, Time elapsed [s]: 19.10\n",
      "Epoch 5, Train Loss: 157.20, Val Loss: 987.54, Time elapsed [s]: 19.13\n",
      "Epoch 6, Train Loss: 140.02, Val Loss: 872.35, Time elapsed [s]: 19.18\n",
      "Epoch 7, Train Loss: 130.14, Val Loss: 900.82, Time elapsed [s]: 19.07\n",
      "Epoch 8, Train Loss: 129.00, Val Loss: 922.06, Time elapsed [s]: 19.15\n",
      "Epoch 9, Train Loss: 136.97, Val Loss: 784.94, Time elapsed [s]: 19.11\n",
      "Epoch 10, Train Loss: 119.26, Val Loss: 812.80, Time elapsed [s]: 19.03\n",
      "Epoch 11, Train Loss: 116.38, Val Loss: 847.16, Time elapsed [s]: 19.15\n",
      "Epoch 12, Train Loss: 118.87, Val Loss: 919.68, Time elapsed [s]: 19.16\n",
      "Epoch 13, Train Loss: 119.20, Val Loss: 748.86, Time elapsed [s]: 19.20\n",
      "Epoch 14, Train Loss: 119.05, Val Loss: 767.22, Time elapsed [s]: 19.03\n",
      "Epoch 15, Train Loss: 115.74, Val Loss: 739.53, Time elapsed [s]: 19.09\n",
      "Epoch 16, Train Loss: 116.06, Val Loss: 719.11, Time elapsed [s]: 19.08\n",
      "Epoch 17, Train Loss: 110.78, Val Loss: 720.13, Time elapsed [s]: 18.97\n",
      "Epoch 18, Train Loss: 107.16, Val Loss: 728.88, Time elapsed [s]: 19.08\n",
      "Epoch 19, Train Loss: 104.39, Val Loss: 734.29, Time elapsed [s]: 19.08\n",
      "Epoch 20, Train Loss: 101.21, Val Loss: 779.78, Time elapsed [s]: 19.05\n",
      "Epoch 21, Train Loss: 108.16, Val Loss: 769.05, Time elapsed [s]: 19.23\n",
      "Epoch 22, Train Loss: 109.12, Val Loss: 752.25, Time elapsed [s]: 19.04\n",
      "Epoch 23, Train Loss: 110.12, Val Loss: 655.00, Time elapsed [s]: 19.09\n",
      "Epoch 24, Train Loss: 96.64, Val Loss: 706.44, Time elapsed [s]: 19.06\n",
      "Epoch 25, Train Loss: 98.03, Val Loss: 708.87, Time elapsed [s]: 18.98\n",
      "Epoch 26, Train Loss: 96.80, Val Loss: 731.75, Time elapsed [s]: 19.07\n",
      "Epoch 27, Train Loss: 93.50, Val Loss: 607.03, Time elapsed [s]: 19.13\n",
      "Epoch 28, Train Loss: 83.15, Val Loss: 634.29, Time elapsed [s]: 19.13\n",
      "Epoch 29, Train Loss: 83.84, Val Loss: 786.41, Time elapsed [s]: 19.04\n",
      "Epoch 30, Train Loss: 105.21, Val Loss: 734.69, Time elapsed [s]: 19.00\n",
      "Epoch 31, Train Loss: 105.94, Val Loss: 642.16, Time elapsed [s]: 19.14\n",
      "Epoch 32, Train Loss: 94.34, Val Loss: 693.34, Time elapsed [s]: 19.07\n",
      "Epoch 33, Train Loss: 90.96, Val Loss: 606.49, Time elapsed [s]: 19.09\n",
      "Epoch 34, Train Loss: 90.87, Val Loss: 581.49, Time elapsed [s]: 19.16\n",
      "Epoch 35, Train Loss: 83.32, Val Loss: 581.74, Time elapsed [s]: 19.09\n",
      "Epoch 36, Train Loss: 84.99, Val Loss: 616.84, Time elapsed [s]: 19.05\n",
      "Epoch 37, Train Loss: 92.06, Val Loss: 606.95, Time elapsed [s]: 19.12\n",
      "Epoch 38, Train Loss: 96.50, Val Loss: 696.78, Time elapsed [s]: 19.06\n",
      "Epoch 39, Train Loss: 109.43, Val Loss: 590.55, Time elapsed [s]: 19.09\n",
      "Epoch 40, Train Loss: 96.35, Val Loss: 614.54, Time elapsed [s]: 19.04\n",
      "Epoch 41, Train Loss: 95.63, Val Loss: 575.59, Time elapsed [s]: 19.03\n",
      "Epoch 42, Train Loss: 91.12, Val Loss: 583.47, Time elapsed [s]: 19.06\n",
      "Epoch 43, Train Loss: 82.84, Val Loss: 622.20, Time elapsed [s]: 19.14\n",
      "Epoch 44, Train Loss: 102.37, Val Loss: 596.80, Time elapsed [s]: 19.07\n",
      "Epoch 45, Train Loss: 83.92, Val Loss: 607.31, Time elapsed [s]: 19.05\n",
      "Epoch 46, Train Loss: 88.23, Val Loss: 673.83, Time elapsed [s]: 19.06\n",
      "Epoch 47, Train Loss: 91.57, Val Loss: 582.91, Time elapsed [s]: 19.16\n",
      "Epoch 48, Train Loss: 101.13, Val Loss: 636.65, Time elapsed [s]: 19.12\n",
      "Epoch 49, Train Loss: 86.27, Val Loss: 575.17, Time elapsed [s]: 19.13\n",
      "Epoch 0, Train Loss: 263.60, Val Loss: 1648.26, Time elapsed [s]: 19.11\n",
      "Epoch 1, Train Loss: 274.13, Val Loss: 1936.99, Time elapsed [s]: 19.11\n",
      "Epoch 2, Train Loss: 209.85, Val Loss: 1620.97, Time elapsed [s]: 19.10\n",
      "Epoch 3, Train Loss: 187.09, Val Loss: 1450.13, Time elapsed [s]: 19.14\n",
      "Epoch 4, Train Loss: 175.84, Val Loss: 1250.98, Time elapsed [s]: 19.01\n",
      "Epoch 5, Train Loss: 159.72, Val Loss: 1216.38, Time elapsed [s]: 19.07\n",
      "Epoch 6, Train Loss: 154.43, Val Loss: 1352.74, Time elapsed [s]: 19.08\n",
      "Epoch 7, Train Loss: 183.65, Val Loss: 1161.78, Time elapsed [s]: 19.04\n",
      "Epoch 8, Train Loss: 156.40, Val Loss: 1185.85, Time elapsed [s]: 19.16\n",
      "Epoch 9, Train Loss: 142.29, Val Loss: 1241.88, Time elapsed [s]: 19.11\n",
      "Epoch 10, Train Loss: 135.50, Val Loss: 1176.29, Time elapsed [s]: 19.08\n",
      "Epoch 11, Train Loss: 135.92, Val Loss: 924.16, Time elapsed [s]: 19.10\n",
      "Epoch 12, Train Loss: 125.75, Val Loss: 1020.76, Time elapsed [s]: 19.06\n",
      "Epoch 13, Train Loss: 116.63, Val Loss: 885.39, Time elapsed [s]: 19.02\n",
      "Epoch 14, Train Loss: 112.32, Val Loss: 847.05, Time elapsed [s]: 19.01\n",
      "Epoch 15, Train Loss: 116.56, Val Loss: 768.14, Time elapsed [s]: 19.07\n",
      "Epoch 16, Train Loss: 120.09, Val Loss: 723.90, Time elapsed [s]: 19.02\n",
      "Epoch 17, Train Loss: 110.66, Val Loss: 748.90, Time elapsed [s]: 19.02\n",
      "Epoch 18, Train Loss: 106.49, Val Loss: 841.92, Time elapsed [s]: 19.09\n",
      "Epoch 19, Train Loss: 103.38, Val Loss: 975.75, Time elapsed [s]: 19.03\n",
      "Epoch 20, Train Loss: 111.81, Val Loss: 788.60, Time elapsed [s]: 19.01\n",
      "Epoch 21, Train Loss: 114.68, Val Loss: 675.64, Time elapsed [s]: 19.02\n",
      "Epoch 22, Train Loss: 107.94, Val Loss: 698.53, Time elapsed [s]: 19.06\n",
      "Epoch 23, Train Loss: 103.34, Val Loss: 712.73, Time elapsed [s]: 19.04\n",
      "Epoch 24, Train Loss: 98.45, Val Loss: 719.97, Time elapsed [s]: 19.01\n",
      "Epoch 25, Train Loss: 106.60, Val Loss: 774.29, Time elapsed [s]: 18.94\n",
      "Epoch 26, Train Loss: 98.88, Val Loss: 669.68, Time elapsed [s]: 18.99\n",
      "Epoch 27, Train Loss: 95.72, Val Loss: 668.93, Time elapsed [s]: 19.01\n",
      "Epoch 28, Train Loss: 99.87, Val Loss: 690.45, Time elapsed [s]: 19.06\n",
      "Epoch 29, Train Loss: 96.87, Val Loss: 602.09, Time elapsed [s]: 19.04\n",
      "Epoch 30, Train Loss: 91.10, Val Loss: 620.64, Time elapsed [s]: 18.98\n",
      "Epoch 31, Train Loss: 87.63, Val Loss: 597.02, Time elapsed [s]: 18.97\n",
      "Epoch 32, Train Loss: 89.16, Val Loss: 634.40, Time elapsed [s]: 19.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_EPOCHS= 50\n",
    "save_path_model= './models/Farrow/finetuned/soma_centered/finetuned_scaled_vae_frac%.1f_best_run%i.pt'\n",
    "save_path_losses = './models/Farrow/finetuned/soma_centered/finetuned_scaled_losses_frac%.1f_run%i.npy'\n",
    "save_path_elapsed_time = './models/Farrow/finetuned/soma_centered/finetuned_scaled_elapsed_time_frac%.1f_run%i.npy'\n",
    "# state_dict = torch.load('./models/5_populations/emb32_hid32_lat32_dp0.1_k500_max_frac1.0_scaled_sum_run1_best.pt')\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "for frac in [1., .9, .5, .1, 0.]:\n",
    "    \n",
    "  \n",
    "    runs = range(1,4)\n",
    "        \n",
    "    for run in runs:\n",
    "        \n",
    "        #optimizer\n",
    "        optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "        \n",
    "        if os.path.exists(save_path_model%(frac,run)):\n",
    "            state_dict = torch.load(save_path_model%(frac,run))\n",
    "            \n",
    "            # load model\n",
    "            model.load_state_dict(state_dict['model_state_dict'])\n",
    "            \n",
    "            # overwrite optimizer if the model had been trained already\n",
    "            optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "            classifier.load_state_dict(state_dict['classifier_state_dict'])\n",
    "            losses = np.load(save_path_losses%(frac, run))\n",
    "            elapsed_time = np.load(save_path_elapsed_time%(frac, run))\n",
    "            \n",
    "            last_epoch = state_dict['epoch']\n",
    "            training = list(losses[:last_epoch,:2])\n",
    "            validation = list(losses[:last_epoch,2:])\n",
    "            elapsed_time = elapsed_time[:last_epoch]\n",
    "            elapsed_time = np.hstack((elapsed_time, np.zeros((N_EPOCHS))))\n",
    "            best_test_loss = losses[:,2].min()\n",
    "            \n",
    "        else:\n",
    "            # load pre-trained model\n",
    "            state_dict = torch.load('./models/Farrow/scratch/soma_centered/vae_frac0.0_scaled_best_run%i.pt'%run)\n",
    "            # the first run was the best\n",
    "            model.load_state_dict(state_dict['model_state_dict'])\n",
    "            classifier.apply(init_weights)\n",
    "            best_test_loss = np.infty\n",
    "\n",
    "            losses = np.load('./models/Farrow/scratch/soma_centered/losses_frac0.0_scaled_run%i.npy'%run)\n",
    "            elapsed_time = np.load('./models/Farrow/scratch/soma_centered/elapsed_time_frac0.0_scaled_run%i.npy'%run)\n",
    "            last_epoch = len(elapsed_time)\n",
    "            elapsed_time = np.hstack((elapsed_time, np.zeros((N_EPOCHS))))\n",
    "            training = list(losses[:,:2])\n",
    "            validation = list(losses[:,2:])\n",
    "        \n",
    "        \n",
    "        cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=-100)\n",
    "        mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        for e in range(N_EPOCHS):\n",
    "            start.record()\n",
    "            train_loss, train_class_loss = train(model, classifier, train_iterator, optimizer, \n",
    "                                               calculate_loss,cross_entropy_loss, clip=1, norm_p=None,\n",
    "                                                 class_fraction=frac)\n",
    "            val_loss, val_class_loss = evaluate(model,classifier, val_iterator,\n",
    "                                                 calculate_loss, cross_entropy_loss, norm_p=None)\n",
    "\n",
    "            train_loss /= N_train\n",
    "            train_class_loss /= N_train\n",
    "            val_loss /= N_val\n",
    "            val_class_loss /=N_val\n",
    "            \n",
    "            end.record()\n",
    "\n",
    "            # Waits for everything to finish running\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time[e+last_epoch] = start.elapsed_time(end) # milliseconds\n",
    "            \n",
    "            training += [[train_loss,train_class_loss]]\n",
    "            validation += [[val_loss, val_class_loss]]\n",
    "            print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}, Time elapsed [s]: {elapsed_time[e]/1000:.2f}')\n",
    "\n",
    "\n",
    "            if e % 50 == 0 and e > 0:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr']/2\n",
    "\n",
    "            if best_test_loss > val_loss:\n",
    "                best_test_loss = val_loss\n",
    "                torch.save({'epoch': e + last_epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'classifier_state_dict': classifier.state_dict()\n",
    "                               },save_path_model%(frac, run))\n",
    "\n",
    "                validation_ = np.array(validation)\n",
    "                training_ = np.array(training)\n",
    "                # [:,0] = training loss, [:,1] = training classification loss \n",
    "                # [:,2] validation loss, [:,3] validation classification loss\n",
    "                losses = np.hstack((training_, validation_))\n",
    "                np.save(save_path_losses%(frac, run),losses)\n",
    "                np.save(save_path_elapsed_time%(frac,run),elapsed_time)\n",
    "        validation = np.array(validation)\n",
    "        training = np.array(training)\n",
    "        losses = np.hstack((training, validation))\n",
    "        np.save(save_path_losses%(frac, run), losses)\n",
    "        np.save(save_path_elapsed_time%(frac,run),elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(validation)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.72294771484375"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time.mean()/1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
