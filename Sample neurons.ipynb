{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "from utils.vmf_batch import vMF\n",
    "from models import SeqEncoder, SeqDecoder, Seq2Seq_VAE, PoolingClassifier\n",
    "from utils.cluster_utils import _convert_cluster_results_dict_into_array, get_clustered_rws_agglom, tree_from_clustered_result\n",
    "from utils.sampling_utils import _fill_with_infty, decode_z, sample_rws\n",
    "\n",
    "## plotting ###\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/toy_data/3_populations/walk_representation_32.npy', 'rb') as f:\n",
    "    walk_representation = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/toy_data/3_populations/iterator/test_iterator.pkl', 'rb') as f:\n",
    "    test_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "# get data\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "src_data, trg_data, seq_len, indices, labels = list(test_iterator)[0]\n",
    "rw_i = np.round(trg_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 18.465579986572266\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "INPUT_DIM = 3\n",
    "EMBED_DIM = 16\n",
    "HIDDEN_DIM = 16\n",
    "LATENT_DIM = 8\n",
    "NUM_LAYERS = 2\n",
    "KAPPA = 500\n",
    "DROPOUT =.1\n",
    "\n",
    "# model\n",
    "enc = SeqEncoder(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "dec = SeqDecoder(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "dist = vMF(LATENT_DIM, kappa=KAPPA, device=device)\n",
    "model = Seq2Seq_VAE(enc, dec, dist, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/parameter_search/emb16_hid16_lat8_dp0.1_k500_avg_run1_best.pt')\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bs, n_walks, walk_length, input_dim = src_data.shape\n",
    "    src = src_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    # src = [walk length , bs * n_walks, input_dim]\n",
    "    trg = trg_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    seq_len = seq_len.view(-1).to(device)\n",
    "    output = model(src, seq_len, trg, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(indices)):\n",
    "    \n",
    "    for kappa in [100,300,500]:\n",
    "        \n",
    "        vmf = vMF(LATENT_DIM, kappa=kappa)\n",
    "        mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "        original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "        decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                                 n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "         # cluster the rws\n",
    "        clustered_rws = []\n",
    "        clustered_results = []\n",
    "        for rws in decoded_rws:\n",
    "            clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.5 )\n",
    "            clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "            clustered_results.append(clus_res)\n",
    "        clustered_rws = np.vstack(clustered_rws)\n",
    "        # reduce to trees\n",
    "        for clus_res in clustered_results:\n",
    "            N = tree_from_clustered_result(clus_res)\n",
    "            N.write_to_swc('%i'%indices[k], path='./data/toy_data/3_populations/sampled_neurons/test_data/v3/k%i/'%kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On real data\n",
    "\n",
    "## M1 EXC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/M1_exc_data/walks/walk_representation.npy', 'rb') as f:\n",
    "    walk_representation = np.load(f)\n",
    "\n",
    "with open('./data/M1_exc_data/iterator/m_labels/test_iterator.pkl', 'rb') as f:\n",
    "    test_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "# get data\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "src_data, trg_data, seq_len, indices, labels = list(test_iterator)[0]\n",
    "rw_i = np.round(trg_data, 2)\n",
    "\n",
    "N, n_walks, walk_length, input_dim = src_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with the best validation loss \n",
    "state_dict = torch.load('./models/M1_exc/m_label/finetuned_vae_k500_frac1.0_best_run2.pt', map_location=device)\n",
    "model.load_state_dict(state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bs, n_walks, walk_length, input_dim = src_data.shape\n",
    "    src = src_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    # src = [walk length , bs * n_walks, input_dim]\n",
    "    trg = trg_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    seq_len = seq_len.view(-1).to(device)\n",
    "    %timeit -r 3 -n 10 output = model(src, seq_len, trg, 0)\n",
    "    output = model(src, seq_len, trg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### time the code: ####\n",
    "timing = []\n",
    "k = 0\n",
    "kappa = 500\n",
    "\n",
    "vmf = vMF(LATENT_DIM, kappa=kappa, device=device)\n",
    "mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "\n",
    "o = %timeit -r 3 -n 100 -o decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "timing.append(('sampling', o))\n",
    "\n",
    "decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                         n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "# cluster the rws\n",
    "clustered_rws = []\n",
    "clustered_results = []\n",
    "for rws in decoded_rws:\n",
    "    o = %timeit -r 3 -o clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "    timing.append(('clustering', o))\n",
    "    clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "    \n",
    "    clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "    clustered_results.append(clus_res)\n",
    "clustered_rws = np.vstack(clustered_rws)\n",
    "# reduce to trees\n",
    "for clus_res in clustered_results:\n",
    "    o = %timeit -r 3 -o N = tree_from_clustered_result(clus_res)\n",
    "    timing.append(('get_tree', o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in range(len(indices)):\n",
    "    \n",
    "    for kappa in [100,300,500]:\n",
    "        \n",
    "        vmf = vMF(LATENT_DIM, kappa=kappa, device=device)\n",
    "        mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "        original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "        decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                                 n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "         # cluster the rws\n",
    "        clustered_rws = []\n",
    "        clustered_results = []\n",
    "        for rws in decoded_rws:\n",
    "            clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "            clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "            clustered_results.append(clus_res)\n",
    "        clustered_rws = np.vstack(clustered_rws)\n",
    "        # reduce to trees\n",
    "        for clus_res in clustered_results:\n",
    "            N = tree_from_clustered_result(clus_res)\n",
    "            N.write_to_swc('%i'%indices[k], path='./data/M1_exc_data/sampled_neurons/test_data/k%i/'%kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 Inh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/M1_inh_data/walks/axon/walk_representation_32.npy', 'rb') as f:\n",
    "    walk_representation = np.load(f)\n",
    "\n",
    "with open('./data/M1_inh_data/iterator/axon/test_iterator_32.pkl', 'rb') as f:\n",
    "    test_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "# get data\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "src_data, trg_data, seq_len, indices, labels = list(test_iterator)[0]\n",
    "rw_i = np.round(trg_data, 2)\n",
    "\n",
    "N, n_walks, walk_length, input_dim = src_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model with the best validation loss \n",
    "state_dict = torch.load('./models/M1_inh/finetuned/axon/finetuned_vae_frac0.5_best_run2.pt',map_location=device)\n",
    "model.load_state_dict(state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bs, n_walks, walk_length, input_dim = src_data.shape\n",
    "    src = src_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    # src = [walk length , bs * n_walks, input_dim]\n",
    "    trg = trg_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    seq_len = seq_len.view(-1).to(device)\n",
    "    %timeit -r 3 -n 10 output = model(src, seq_len, trg, 0)\n",
    "    output = model(src, seq_len, trg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### time the code: ####\n",
    "k = 0\n",
    "kappa = 500\n",
    "\n",
    "vmf = vMF(LATENT_DIM, kappa=kappa, device=device)\n",
    "mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "\n",
    "o = %timeit -r 3 -n 100 -o decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "timing.append(('inh', 'sampling', o))\n",
    "\n",
    "decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                         n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "# cluster the rws\n",
    "clustered_rws = []\n",
    "clustered_results = []\n",
    "for rws in decoded_rws:\n",
    "    o = %timeit -r 3 -o clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "    timing.append(('inh','clustering', o))\n",
    "    clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "    \n",
    "    clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "    clustered_results.append(clus_res)\n",
    "clustered_rws = np.vstack(clustered_rws)\n",
    "# reduce to trees\n",
    "for clus_res in clustered_results:\n",
    "    o = %timeit -r 3 -o N = tree_from_clustered_result(clus_res)\n",
    "    timing.append(('inh', 'get_tree', o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(indices)):\n",
    "    \n",
    "    for kappa in [100,300,500]:\n",
    "        \n",
    "        vmf = vMF(LATENT_DIM, kappa=kappa)\n",
    "        mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "        original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "        decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                                 n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "         # cluster the rws\n",
    "        clustered_rws = []\n",
    "        clustered_results = []\n",
    "        for rws in decoded_rws:\n",
    "            clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.3)\n",
    "            clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "            clustered_results.append(clus_res)\n",
    "        clustered_rws = np.vstack(clustered_rws)\n",
    "        # reduce to trees\n",
    "        for clus_res in clustered_results:\n",
    "            N = tree_from_clustered_result(clus_res)\n",
    "            N.write_to_swc('%i'%indices[k], path='./data/M1_inh_data/sampled_neurons/axon/test_data/k%i/'%kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Farrow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 'soma_centered'\n",
    "\n",
    "with open('./data/Farrow_data/walks/%s/walk_representation.npy'%part, 'rb') as f:\n",
    "    walk_representation = np.load(f)\n",
    "\n",
    "with open('./data/Farrow_data/iterator/%s/test_iterator.pkl'%part, 'rb') as f:\n",
    "    test_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 17\n",
    "\n",
    "# get data\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "src_data, trg_data, seq_len, indices, labels = list(test_iterator)[0]\n",
    "rw_i = np.round(trg_data, 2)\n",
    "\n",
    "N, n_walks, walk_length, input_dim = src_data.shape\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = dict(input_dim =3, embed_dim=16, hidden_dim=16, latent_dim=8, num_layers = 2, kappa=500, dropout=.1)\n",
    "\n",
    "LATENT_DIM = config['latent_dim']\n",
    "\n",
    "# model with the best validation mse loss \n",
    "state_dict = torch.load('./models/Farrow/finetuned/%s/finetuned_vae_frac1.0_best_run1.pt'%part, map_location=device)\n",
    "model.load_state_dict(state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62 s ± 4.78 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    bs, n_walks, walk_length, input_dim = src_data.shape\n",
    "    src = src_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    # src = [walk length , bs * n_walks, input_dim]\n",
    "    trg = trg_data.view(-1,walk_length,input_dim).transpose(0,1).to(device)\n",
    "    seq_len = seq_len.view(-1).to(device)\n",
    "    %timeit -r 3 -n 10 output = model(src, seq_len, trg, 0)\n",
    "    output = model(src, seq_len, trg, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 18.465579986572266\n",
      "97.6 ms ± 459 µs per loop (mean ± std. dev. of 3 runs, 100 loops each)\n",
      "28.6 ms ± 13.9 µs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "3.87 ms ± 7.91 µs per loop (mean ± std. dev. of 3 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# time the code\n",
    "\n",
    "### time the code: ####\n",
    "k = 0\n",
    "kappa = 500\n",
    "timing = []\n",
    "vmf = vMF(LATENT_DIM, kappa=kappa, device=device)\n",
    "mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "\n",
    "o = %timeit -r 3 -n 100 -o decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "timing.append(('rgc', 'sampling', o))\n",
    "\n",
    "decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                         n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "# cluster the rws\n",
    "clustered_rws = []\n",
    "clustered_results = []\n",
    "for rws in decoded_rws:\n",
    "    o = %timeit -r 3 -o clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "    timing.append(('rgc','clustering', o))\n",
    "    clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.4 )\n",
    "    \n",
    "    clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "    clustered_results.append(clus_res)\n",
    "clustered_rws = np.vstack(clustered_rws)\n",
    "# reduce to trees\n",
    "for clus_res in clustered_results:\n",
    "    o = %timeit -r 3 -o N = tree_from_clustered_result(clus_res)\n",
    "    timing.append(('rgc', 'get_tree', o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n",
      "KLD: 11.350215911865234\n",
      "KLD: 16.18794822692871\n",
      "KLD: 18.465579986572266\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(indices)):\n",
    "    \n",
    "    for kappa in [100,300,500]:\n",
    "        \n",
    "        vmf = vMF(LATENT_DIM, kappa=kappa)\n",
    "        mus = model.h[k*n_walks: k*n_walks+n_walks]\n",
    "        original_seq_len = seq_len[k*n_walks: k*n_walks+n_walks].cpu()\n",
    "        decoded_rws = sample_rws(model, vmf, mus, orig_seq_len=original_seq_len,\n",
    "                                 n_samples=1, max_trg_len=walk_length, min_angle=np.pi/2.4)\n",
    "\n",
    "         # cluster the rws\n",
    "        clustered_rws = []\n",
    "        clustered_results = []\n",
    "        for rws in decoded_rws:\n",
    "            clus_res, clus_rws = get_clustered_rws_agglom(rws,dist_thresh=.25)\n",
    "            clustered_rws.append(clus_rws.reshape((1,)+clus_rws.shape))\n",
    "            clustered_results.append(clus_res)\n",
    "        clustered_rws = np.vstack(clustered_rws)\n",
    "        # reduce to trees\n",
    "        for clus_res in clustered_results:\n",
    "            N = tree_from_clustered_result(clus_res)\n",
    "            N.write_to_swc('%i'%indices[k], path='./data/Farrow_data/sampled_neurons/soma_centered/test_data/k%i/'%kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urban data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 'soma_centered'\n",
    "\n",
    "with open('./data/urban_data/walks/%s/walk_representation_16.npy'%part, 'rb') as f:\n",
    "    walk_representation = np.load(f)\n",
    "\n",
    "with open('./data/urban_data/iterator/%s/test_iterator.pkl'%part, 'rb') as f:\n",
    "    test_iterator = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "\n",
    "# get data\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "src_data, trg_data, seq_len, indices, labels = list(test_iterator)[0]\n",
    "rw_i = np.round(trg_data, 2)\n",
    "\n",
    "N, n_walks, walk_length, input_dim = src_data.shape\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = dict(input_dim =3, embed_dim=16, hidden_dim=16, latent_dim=8, num_layers = 2, kappa=500, dropout=.1)\n",
    "\n",
    "LATENT_DIM = config['latent_dim']\n",
    "\n",
    "# model with the best validation mse loss \n",
    "state_dict = torch.load('./models/urban/finetuned/%s/finetuned_vae_frac1.0_best_run1.pt'%part, map_location=device)\n",
    "model.load_state_dict(state_dict['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
